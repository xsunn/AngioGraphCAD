{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('')\n",
    "import torch\n",
    "from maskAttention import *\n",
    "from predictionModel import *\n",
    "from gle import *\n",
    "import configurationGIN as cfg\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNClassificationModule(\n",
      "  (model): GNNClassifier(\n",
      "    (gnn): GIN(3, 256, num_layers=6)\n",
      "    (polling): ConfigurablePooling(\n",
      "      (lin1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): ReLU()\n",
      "        (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      )\n",
      "      (lin2): Sequential(\n",
      "        (0): Linear(in_features=85, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): ReLU()\n",
      "        (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        (12): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--nb_classes\", type=int, default=1, help=\"The number of classes to predict for our clf task\")\n",
    "# This is automatically set from the dataset module\n",
    "parser.add_argument(\"--gnn_node_features\", type=int, default=3, help=\"Dim of features in the nodes that we will feed to GNN\")\n",
    "\n",
    "# GNN model parameters\n",
    "# -- pooling\n",
    "parser.add_argument(\"--gnn_pooling_cls_name\", type=str, default=\"ConfigurablePooling\", help=\"One of pooling layers define din 'cardio.networks.gnn' \")\n",
    "parser.add_argument(\"--pool_only_lesion_points\", action='store_true', help=\"In case we use whole artery, gnn pooling will pool only lesion points\")\n",
    "parser.add_argument(\"--node_pooling\", type=str, default='mean', help=\"How to pool the nodes of the gnn: sum, mean, max, all_stats\")\n",
    "# -- model\n",
    "parser.add_argument(\"--gnn_is_siamese\", default=False, help=\"In case we use siamese dataset use this flag\")\n",
    "parser.add_argument(\"--gnn_cls_name\", type=str, default=\"GIN\", help=\"One of the pred class names in 'torch_geometric.nn.models.basic_gnn' or one of our custom gnn class names, CustomGIN\")\n",
    "parser.add_argument(\"--gnn_node_emb_dim\", type=int, default=256, help=\"GNN hidden dim that will be input of the GATconv\")\n",
    "parser.add_argument(\"--gnn_global_hidden_dim\", type=int, default=512, help=\"GNN The dimension to have after the pooling \")\n",
    "parser.add_argument(\"--gnn_nb_layers\", type=int, default=6, help=\"GNN number of Transformer layers\")\n",
    "parser.add_argument(\"--gnn_dropout\", type=float, default=0.415081799988646554, help=\"GNN The dropout on the Attention layers of the gnn\")\n",
    "parser.add_argument(\"--gnn_act\", type=str, default=\"relu\", help=\"The type of activation (one of torch activations)\")\n",
    "parser.add_argument(\"--gnn_norm\", type=str, default=\"BatchNorm\", help=\"One of the normalizations, see 'torch_geometric.nn.norm.__init__.py'\")\n",
    "parser.add_argument(\"--gnn_freeze_weights\", type=bool, default=False, help=\"Freeze gnn weights (not pooling)\")\n",
    "parser.add_argument(\"--gnn_ffr_pooling_factor\", type=int, default=1, help=\"Scale the global dim by a factor multiple times (until dim is >= 16) before you concat it with ffr\")\n",
    "parser.add_argument(\"--gnn_ffr_pooling_proj_dim\", type=int, default=16, help=\"Stops scalling when you reach this projection dimention\")\n",
    "parser.add_argument(\"--use_lesion_wide_info\", default=True,  help=\"Uses lesion wide features like FFR and DS\")\n",
    "# parser.add_argument(\"--nb_clinical_data_features\", default=24, type=int, help=\"The number of clinical data features, if used with ClinicalDataPooling\")\n",
    "parser.add_argument(\"--lesion_wide_feat_dim\", default=21, type=int, help=\"The number of clinical data features, if used with ClinicalDataPooling\")\n",
    "# parser.add_argument(\"--gnn_pooling_cls_name\", type=str, default=\"ConfigurablePooling\", help=\"One of pooling layers define din 'cardio.networks.gnn' \")\n",
    "\n",
    "confs = parser.parse_args(args=[])\n",
    "\n",
    "backbone = GNNClassificationModule(confs)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergModule(\n",
      "  (enc): GNNClassificationModule(\n",
      "    (model): GNNClassifier(\n",
      "      (gnn): GIN(3, 256, num_layers=6)\n",
      "      (polling): ConfigurablePooling(\n",
      "        (lin1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "          (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "          (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (10): ReLU()\n",
      "          (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "        )\n",
      "        (lin2): Sequential(\n",
      "          (0): Linear(in_features=85, out_features=32, bias=True)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "          (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "          (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (10): ReLU()\n",
      "          (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "          (12): Linear(in_features=16, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sinconPos): SinconPos()\n",
      "  (fusion): MAM(\n",
      "    (learnablePos): LearnablePositionalEncoding()\n",
      "    (norm_att): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (k): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (v): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (att_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): GELU(approximate=none)\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ff_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (fusionNetwork): MAL(\n",
      "    (layers): ModuleList(\n",
      "      (0): MAM(\n",
      "        (learnablePos): LearnablePositionalEncoding()\n",
      "        (norm_att): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (q): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (k): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (v): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (att_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): GELU(approximate=none)\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ff_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): MAM(\n",
      "        (learnablePos): LearnablePositionalEncoding()\n",
      "        (norm_att): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (q): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (k): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (v): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (att_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): GELU(approximate=none)\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ff_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): MAM(\n",
      "        (learnablePos): LearnablePositionalEncoding()\n",
      "        (norm_att): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (q): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (k): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (v): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (att_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): GELU(approximate=none)\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ff_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): MAM(\n",
      "        (learnablePos): LearnablePositionalEncoding()\n",
      "        (norm_att): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (q): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (k): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (v): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (att_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): GELU(approximate=none)\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ff_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (clic): Sequential(\n",
      "    (0): Linear(in_features=27, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (preHeader): Sequential(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (11): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (12): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (14): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (17): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (18): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    (21): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      "  (norma): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "confs.lesionNum =9\n",
    "confs.fusion_method ='atten'\n",
    "\n",
    "patient_model=mergModule(backbone, confs,lesionNum=confs.lesionNum, embedding_dim=32,dim_feedforward=32, nhead=4,num_layers=4,qkv_bias=False,drop_rate=0.3)\n",
    "print(patient_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
