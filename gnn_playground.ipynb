{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check new pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.preprocess.graph import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    \n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    generate_data_with_2_views=True,\n",
    "    duplicate_imgs_with_multiple_lesions=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = fame2_ds.df.patient_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds = Fame2RawDSLoader(\n",
    "    \n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    keep_2views_data_for_comparison=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    duplicate_imgs_with_multiple_lesions=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_x = fame2_ds.df.patient_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "def create_graph(coordinates, edge_index):\n",
    "    g = ig.Graph(len(coordinates))\n",
    "    g.vs['x'] = coordinates[:,0].tolist()\n",
    "    g.vs['y'] = coordinates[:,1].tolist()\n",
    "\n",
    "    coords = g.layout_auto().coords\n",
    "    for edge in edge_index:\n",
    "        g.add_edge(edge[0], edge[1])\n",
    "    g.simplify() \n",
    "\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot graph generated from pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ImageToGraphPipeline(\n",
    "    SimplePixelExtractor(True, False, ['FFR', 'exactSegmentLocation'], [None, artery_segment_to_float]), \n",
    "    StochasticDownsampler(True), \n",
    "    KNNVertexConnector(5),\n",
    "    # DelaunayVertexConnector(10),\n",
    "    TASK_TO_LABEL_EXTRACTOR['EventForecast'],\n",
    "    0.5, 1\n",
    ")\n",
    "\n",
    "ds_item = fame2_ds[100]\n",
    "\n",
    "coordinates, features, _, _ = pipeline.generate_sampled_vertices(ds_item)\n",
    "edge_index = pipeline.vertex_connector.connect(coordinates, features)\n",
    "print(features.shape)\n",
    "\n",
    "f, ax = plt.subplots(1,4, figsize=(20,5))\n",
    "ax[0].scatter(coordinates[:,0], coordinates[:,1])\n",
    "\n",
    "\n",
    "g = create_graph(coordinates, edge_index)\n",
    "ig.plot(\n",
    "    g,\n",
    "    target=ax[1],\n",
    "    vertex_size=0.04,\n",
    "    vertex_color=\"lightblue\",\n",
    "    edge_width=0.8\n",
    ")\n",
    "\n",
    "ax[2].imshow(ds_item[0][0])\n",
    "ax[3].imshow(ds_item[1])\n",
    "print(\"Label:\", pipeline.point_label_extractor_callback(ds_item[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.dataset import Fame2GraphDatasetWrapper\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from cardio.networks.gnn import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=\"data/datasets/eventForecastingKnnOnlyLesionsWFFR/train\")\n",
    "dl = DataLoader(ds, batch_size = 2)\n",
    "\n",
    "it = iter(dl)\n",
    "batch = next(it)\n",
    "\n",
    "# x = torch.zeros((batch.x.shape[0], 256))\n",
    "# aux_feats = batch.x[:, 3:]\n",
    "# batch_index = batch.batch\n",
    "# batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.vstack([g.lesion_wide_feat_tensor for g in ds]).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random classifiers predictions\n",
    "\n",
    "Predictions for Event Forecasting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.dataset import Fame2GraphDatasetWrapper\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import torchmetrics.classification as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# from cardio.networks.gnn import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "test_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=\"data/datasets/eventForecastDelaunayOnlyLesions/test\")\n",
    "\n",
    "\n",
    "# Create a random test prediction using the train set proba distr\n",
    "train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=\"data/datasets/eventForecastDelaunayOnlyLesions/train\")\n",
    "train_counts = train_ds.data.y.unique(return_counts=True)\n",
    "classes, probas = train_counts[0], train_counts[1] / train_counts[1].sum()\n",
    "\n",
    "test_metrics = [metrics.BinaryAccuracy(), metrics.BinaryPrecision(), metrics.BinaryRecall(), metrics.BinaryF1Score(), metrics.Specificity(), \n",
    "    metrics.AUROC()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uniformly 1 and 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Plot all scores\n",
    "runs = 10\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for r in range(runs):\n",
    "    for m in test_metrics:\n",
    "        pred = torch.rand(test_ds.data.y.shape)\n",
    "        metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred, test_ds.data.y.to(torch.long)).item())\n",
    "        m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f} (+- {np.std(v):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on train label distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "runs = 10\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for r in range(runs):\n",
    "    for m in test_metrics:\n",
    "        pred = torch.from_numpy(np.random.choice(classes, size=test_ds.data.y.shape, p=probas.numpy()))\n",
    "        metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred.to(torch.float), test_ds.data.y.to(torch.long)).item())  \n",
    "        m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f} (+- {np.std(v):.2f})\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for Lesion Detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.pylightning as plmods\n",
    "\n",
    "train_dataset_dir = \"data/datasets/lesionDetectionDelaunay/train\"\n",
    "test_dataset_dir = \"data/datasets/lesionDetectionDelaunay/test\"\n",
    "\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "test_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=test_dataset_dir)\n",
    "\n",
    "\n",
    "# Create a random test prediction using the train set proba distr\n",
    "train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=train_dataset_dir)\n",
    "train_counts = train_ds.data.y.unique(return_counts=True)\n",
    "classes, probas = train_counts[0], train_counts[1] / train_counts[1].sum()\n",
    "\n",
    "test_metrics = [metrics.BinaryAccuracy(), metrics.BinaryPrecision(), metrics.BinaryRecall(), metrics.BinaryF1Score(), \n",
    "    metrics.AUROC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as metrics\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "runs = 10\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for r in range(runs):\n",
    "    for m in test_metrics:\n",
    "        pred = torch.rand(test_ds.data.y.shape)\n",
    "        metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred,test_ds.data.y.to(torch.long)).item())\n",
    "        m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f} (+- {np.std(v):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "runs = 10\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for r in range(runs):\n",
    "    for m in test_metrics:\n",
    "        pred = torch.from_numpy(np.random.choice(classes, size=test_ds.data.y.shape, p=probas.numpy()))\n",
    "        metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred.to(torch.float),test_ds.data.y.to(torch.long)).item())  \n",
    "        m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f} (+- {np.std(v):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using only FFR\n",
    "\n",
    "* Use ROC curves on KFOLD to find best ffr threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Conf:\n",
    "    gradient_clipping = None\n",
    "    nb_epochs = 5\n",
    "    eval_every_epochs = 1\n",
    "    log_every_steps = 50\n",
    "    val_batch_size = 8\n",
    "    train_batch_size = 8\n",
    "    num_workers = 4\n",
    "    task = \"forecast\"\n",
    "    checkpoint = False\n",
    "    device = \"cpu\"\n",
    "    model = \"gnn\"\n",
    "    run_mode = \"simple\"\n",
    "    dataset_dir = \"data/datasets/eventForecastWithDiamSten\"\n",
    "\n",
    "confs = Conf()\n",
    "\n",
    "''' Custom kflod fit using pyotrchlighting that returns validation metrics'''\n",
    "folds = 5\n",
    "confs.checkpoint = False # avoit checkpoints on kfold so we dont explode disk\n",
    "\n",
    "# Resolvers\n",
    "datamodule = resolvers.datamodule_resolver(confs)\n",
    "\n",
    "# setup\n",
    "datamodule.setup(None)\n",
    "\n",
    "\n",
    "# datamodule.setup_folds(folds)\n",
    "# Loop for all folds\n",
    "# fold_predictions = [] \n",
    "# for fold in range(folds):\n",
    "#     print(f\"\\n\\n~~~~~ FOLD {fold} ~~~~~\")\n",
    "#     datamodule.setup_fold_index(fold)\n",
    "\n",
    "#     val_y = list() \n",
    "#     val_ffr = list() \n",
    "#     for graph in datamodule.val_fold:\n",
    "#         val_ffr.append( graph.x[:,3][graph.x[:,3] != 0.0][1].item() )\n",
    "#         val_y.append( graph.y.item() )\n",
    "\n",
    "#     fold_predictions.append( (np.array(val_y), np.array(val_ffr)) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fprs, tprs, folds = [], [], []\n",
    "for f, pred in enumerate(fold_predictions):\n",
    "\n",
    "    y = pred[0]\n",
    "    scores = 1-pred[1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, scores, pos_label=1.0) # high FFR means 0 not 1\n",
    "    \n",
    "    assert len(fpr) == len(tpr)\n",
    "    for i in range(len(fpr)):\n",
    "        fprs.append(fpr[i])\n",
    "        tprs.append(tpr[i])\n",
    "        folds.append(f)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], \"--r\")\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"KFold ROC Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(fpr, tpr, c=thresholds)\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], \"--r\")\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC curve for `1-FFR` threshold\")\n",
    "\n",
    "ax = plt.gca()\n",
    "for i, txt in enumerate(thresholds):\n",
    "    ax.annotate(f\"{txt:.2f}\", (fpr[i], tpr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "f, ax = plt.subplots(2,1, figsize=(5, 10))\n",
    "\n",
    "for i, t in enumerate([0.20, 0.30]):\n",
    "    ffr_threshold = t\n",
    "    pred = []\n",
    "    test_y = []\n",
    "    for graph in datamodule.test_dataset:\n",
    "        ffr = graph.x[:,3][graph.x[:,3] != 0.0][1].item()\n",
    "        pred.append(1.0 if 1-ffr >= ffr_threshold else 0.0)\n",
    "        test_y.append(graph.y.item())\n",
    "\n",
    "    cm = confusion_matrix(test_y, pred)\n",
    "    d =ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1]) \n",
    "    ax[i].set_title(f\"CM for FFR {1-t}\")\n",
    "    d.plot(ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(5, 5))\n",
    "\n",
    "ds_threshold = 70\n",
    "pred = []\n",
    "test_y = []\n",
    "for graph in datamodule.test_dataset:\n",
    "    ds = graph.x[:,3][graph.x[:,3] != 0.0][1].item()\n",
    "    pred.append(1.0 if ds >= ds_threshold else 0.0)\n",
    "    test_y.append(graph.y.item())\n",
    "\n",
    "cm = confusion_matrix(test_y, pred)\n",
    "d =ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1]) \n",
    "ax.set_title(f\"CM for Diameter Stenosis Prediction\")\n",
    "d.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ffrs = np.concatenate([t[1] for t in fold_predictions])\n",
    "labels = np.concatenate([t[0] for t in fold_predictions])\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "sns.histplot(x=ffrs, hue=labels, ax= ax, bins=40)\n",
    "ax.axvline(x=0.8, c=\"r\")\n",
    "ax.axvline(x=0.7, c=\"r\")\n",
    "ax.set_title(\"FFR distribution (Hue: events per ffr bar)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "We can see that the best threshold is closer to `0.8` for the data.\n",
    "\n",
    "\n",
    "* Lets predict the test set by using `0.8` as a threshold and plot all measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification as metrics\n",
    "import torchmetrics as tmetrics\n",
    "\n",
    "ffr_threshold = 70\n",
    "pred = []\n",
    "test_y = []\n",
    "for graph in datamodule.test_dataset:\n",
    "    ffr = graph.x[:,3][graph.x[:,3] != 0.0][1].item()\n",
    "    pred.append(1.0 if ffr >= ffr_threshold else 0.0)\n",
    "    test_y.append(graph.y.item())\n",
    "\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "pred = torch.tensor(pred, dtype=torch.float)\n",
    "\n",
    "test_metrics = [metrics.BinaryAccuracy(), metrics.BinaryPrecision(), metrics.BinaryRecall(), metrics.BinaryF1Score(), tmetrics.AUROC(2,1), metrics.Specificity()]\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for m in test_metrics:\n",
    "    metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred, test_y).item())  \n",
    "    m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFOLD plots to compare lesion detection w FFR and w/out FFR models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/logs_lesion_wout_ffr.pickle', 'rb') as handle:\n",
    "    lesion_wout_ffr = pickle.load(handle)\n",
    "\n",
    "with open('data/logs_lesion_w_ffr.pickle', 'rb') as handle:\n",
    "    lesion_with_ffr = pickle.load(handle)\n",
    "\n",
    "lesion_wout_ffr.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def compare_model_folds(metric_name, m1_metrics_dict, m2_metrics_dict, m1_name, m2_name, figsize = (10, 5)):\n",
    "    # Model 1 metrics\n",
    "    metric_m1, folds_m1 = zip(*m1_metrics_dict[metric_name])\n",
    "    metric_m1, folds_m1 = [i.item() if isinstance(i, torch.Tensor) else i for i in metric_m1], list(folds_m1)\n",
    "    \n",
    "    cnts_m1 = Counter(folds_m1)\n",
    "    assert len(set(cnts_m1.values())) == 1, \"Folds of m1 have different lenghts\"\n",
    "\n",
    "    # Model 2 losses\n",
    "    metric_m2, folds_m2 = zip(*m2_metrics_dict[metric_name])\n",
    "    metric_m2, folds_m2 = [i.item() if isinstance(i, torch.Tensor) else i for i in metric_m2], list(folds_m2)\n",
    "\n",
    "    cnts_m2 = Counter(folds_m2)\n",
    "    assert len(set(cnts_m2.values())) == 1, \"Folds of m2 have different lenghts\"\n",
    "\n",
    "  \n",
    "    # Plot metric\n",
    "    steps_m1 = [i for _ in range(0,len(cnts_m1)) for i in range(0,cnts_m1['fold-0'])]\n",
    "    steps_m2 = [i for _ in range(0,len(cnts_m2)) for i in range(0,cnts_m2['fold-0'])]\n",
    "    names_m1 =  [m1_name] * len(metric_m1)\n",
    "    names_m2 =  [m2_name] * len(metric_m2)\n",
    "    df = pd.DataFrame(data={\n",
    "        metric_name: metric_m1 + metric_m2, \n",
    "        'Folds': folds_m1 + folds_m2, \n",
    "        'Model': names_m1 + names_m2})\n",
    "    \n",
    "    f, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    if metric_name == \"val/epoch_loss\":\n",
    "        ax.set(yscale=\"log\")\n",
    "    sns.lineplot(data=df, y=metric_name, x= steps_m1 + steps_m2, hue=\"Model\", ax=ax)\n",
    "    ax.set_title(f\"'{m1_name}' VS '{m2_name}' on metric: '{metric_name}'\")\n",
    "\n",
    "\n",
    "m1_name = \"GNN w/out ffr\"\n",
    "m2_name = \"GNN w/ ffr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train loss\n",
    "metric_name = 'train/epoch_loss'\n",
    "compare_model_folds(metric_name, lesion_wout_ffr, lesion_with_ffr, m1_name, m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "metric_name = 'val/epoch_Recall'\n",
    "compare_model_folds(metric_name, lesion_wout_ffr, lesion_with_ffr, m1_name, m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1\n",
    "metric_name = 'val/epoch_F1Score'\n",
    "compare_model_folds(metric_name, lesion_wout_ffr, lesion_with_ffr, m1_name, m2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "with open('logs_storage.pickle', 'rb') as handle:\n",
    "    model_logs = pickle.load(handle)\n",
    "\n",
    "def plot_model_kfold(metric_name, m1_metrics_dict, m1_name, figsize = (10, 5)):\n",
    "    # Model 1 metrics\n",
    "    metric_m1, folds_m1 = zip(*m1_metrics_dict[metric_name])\n",
    "    metric_m1, folds_m1 = [i.item() if isinstance(i, torch.Tensor) else i for i in metric_m1], list(folds_m1)\n",
    "    \n",
    "    cnts_m1 = Counter(folds_m1)\n",
    "    assert len(set(cnts_m1.values())) == 1, \"Folds of m1 have different lenghts\"\n",
    "  \n",
    "    # Plot metric\n",
    "    steps_m1 = [i for _ in range(0,len(cnts_m1)) for i in range(0,cnts_m1['fold-0'])]\n",
    "    names_m1 =  [m1_name] * len(metric_m1)\n",
    "    df = pd.DataFrame(data={\n",
    "        metric_name: metric_m1, \n",
    "        'Folds': folds_m1, \n",
    "        'Model': names_m1})\n",
    "    \n",
    "    f, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    if metric_name == \"val/epoch_loss\":\n",
    "        ax.set(yscale=\"log\")\n",
    "    sns.lineplot(data=df, y=metric_name, x= steps_m1, hue=\"Model\", ax=ax)\n",
    "    ax.set_title(f\"'{m1_name}' on metric: '{metric_name}'\")\n",
    "\n",
    "plot_model_kfold(\"val/epoch_F1Score\", model_logs, \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make FFR a probability distribution to predict Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df_cl = pd.read_csv('data/fame2_clinical_events_2year_data.csv')\n",
    "\n",
    "# Useful columns\n",
    "event_columns = ['VOCE', 'UR_TVF', 'NUR_TVF', 'MI_TVF', 'CV_TVF']\n",
    "clinical_data_patient_lvl = ['CP_CN_MP_MN', 'HTN', 'Hchol', 'DM_Overall',\n",
    "       'DM_Insulin', 'Ren_Ins', 'PVD', 'CVA', 'Prev_MI', 'Prev_PCI', 'CCS',\n",
    "       'CCS_3', 'Silent_Ischemia', 'LVEF', 'MVD', 'Lesion_Type', 'Age', 'Male',\n",
    "       'BMI', 'CAD', 'Smoker', 'Age_64', 'BMI_28']\n",
    "\n",
    "# \n",
    "# Lesion level dataset\n",
    "# \n",
    "df_cl.Prev_MI.fillna(0, inplace=True)\n",
    "df_cl.Lesion_Length.fillna(0, inplace=True)\n",
    "df_cl['Event'] = (df_cl.filter(event_columns).sum(axis=1) > 0).astype(int)\n",
    "\n",
    "def sigmoid(x):\n",
    "       return 1 / (1 + np.exp(-x))\n",
    "df_cl['FFR2Proba'] = df_cl.FFR80.apply(lambda x: sigmoid(0.8 - x)) # FFR > 0.8 -> class 0 so we need to invert (-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "sns.histplot(df_cl.FFR , ax=ax[0])\n",
    "sns.histplot(0.8 - df_cl.FFR, ax=ax[1])\n",
    "\n",
    "ffr = torch.tensor(df_cl.FFR)\n",
    "ffr_sm = torch.nn.functional.sigmoid(0.8 - ffr)\n",
    "sns.histplot(ffr_sm, ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torchmetrics import AUROC, Precision\n",
    "import numpy as np\n",
    "\n",
    "y_true = torch.tensor(df_cl.Event)\n",
    "pred = ffr_sm\n",
    "\n",
    "test_metrics = [BinaryAccuracy(), BinaryPrecision(), BinaryRecall(), BinaryF1Score(), AUROC(2,1)]\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for m in test_metrics:\n",
    "    metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred, y_true).item())  \n",
    "    m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(df_cl.Event)\n",
    "pred = torch.tensor(df_cl.FFR.apply(lambda x: 0.0 if x > 0.8 else 1.0))\n",
    "\n",
    "test_metrics = [BinaryAccuracy(), BinaryPrecision(), BinaryRecall(), BinaryF1Score(), AUROC(2,1)]\n",
    "metrics_runs = {f\"{m.__class__.__name__}\":list() for m in test_metrics }\n",
    "for m in test_metrics:\n",
    "    metrics_runs[f\"{m.__class__.__name__}\"].append(m(pred, y_true).item())  \n",
    "    m.reset()\n",
    "\n",
    "for c,v in metrics_runs.items():\n",
    "    print(f\"{c} = {np.mean(v):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE on node embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from cardio.dataset import Fame2GraphDatasetWrapper\n",
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "CHECKPOINT_PATH_GNN = \"checkpoint/GIN-L1-F3-E128_LesionPooling-G512_E20_EF_DE.ckpt\"\n",
    "\n",
    "class Conf:\n",
    "    gradient_clipping = None\n",
    "    nb_epochs = 5\n",
    "    eval_every_epochs = 1\n",
    "    log_every_steps = 50\n",
    "    val_batch_size = 1 \n",
    "    train_batch_size = 1 \n",
    "    test_batch_size = 1 \n",
    "    num_workers = 4\n",
    "    checkpoint = False\n",
    "    device = \"cpu\"\n",
    "    model = \"gnn\"\n",
    "    run_mode = \"simple\"\n",
    "    train_dataset_dir = \"data/datasets/eventForcastingDelaunay/train\"\n",
    "    test_dataset_dir = \"data/datasets/eventForcastingDelaunay/test\"\n",
    "\n",
    "    # Model configs\n",
    "    gnn_norm = \"BatchNorm\"\n",
    "    gnn_pooling_cls_name = \"LesionPooling\"\n",
    "    weight_init_strategy = \"\"\n",
    "    gnn_cls_name = \"GIN\"\n",
    "    gnn_nb_layers = 1\n",
    "    gnn_dropout = 0.5973039417606918     \n",
    "    gnn_node_emb_dim = 128\n",
    "    gnn_global_hidden_dim = 512\n",
    "    gnn_ffr_pooling_proj_dim = 16\n",
    "    gnn_node_features = 3\n",
    "    gnn_ffr_pooling_factor = 2\n",
    "    gnn_act = 'relu'\n",
    "    nb_classes = 1\n",
    "    weight_init_strategy = \"Xavier Uniform\"\n",
    "    init_weights_from = None\n",
    "    gnn_freeze_weights = False\n",
    "\n",
    "    init_weights_for = 'both'\n",
    "    init_weights_from= CHECKPOINT_PATH_GNN\n",
    "    use_balanced_batches = False\n",
    "    \n",
    "confs = Conf()\n",
    "\n",
    "''' Custom kflod fit using pyotrchlighting that returns validation metrics'''\n",
    "folds = 5\n",
    "confs.checkpoint = False # avoit checkpoints on kfold so we dont explode disk\n",
    "\n",
    "# Resolvers\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.train_dataset_dir)\n",
    "# test_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.test_dataset_dir )\n",
    "model = resolvers.model_resolver(confs, None, None, None).model\n",
    "model.eval()\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot with T-SNE all the nodes in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_vecs = np.zeros((len(train_ds), 128), dtype=np.float32)\n",
    "y_train = np.zeros((len(train_ds), 1), dtype=np.int32)\n",
    "ffr_train = np.zeros((len(train_ds), 1), dtype=np.float32)\n",
    "\n",
    "def model_predict(model, input):\n",
    "    return torch.sigmoid(model(input)).item()\n",
    "\n",
    "# Create Train \n",
    "for i, graph in enumerate(train_ds):\n",
    "    # Prepare data\n",
    "    ffr = graph.x[:,3][graph.x[:,3] != 0.0][1].item() # FFR is in the last column of x, but only exists for lesion nodes\n",
    "\n",
    "    y_train[i] = graph.y.item()\n",
    "    ffr_train[i,0] = ffr\n",
    "\n",
    "    batched_x = graph.x[:, :3]\n",
    "    node_vecs[i, :] = model.gnn(batched_x, graph.edge_index).detach().numpy().mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "nodes_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "    init='random', perplexity=3).fit_transform(node_vecs)\n",
    "\n",
    "plt.scatter(nodes_tsne[:,0], nodes_tsne[:,1], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot multiple graphs on T-SNE to see if lesion pools are are different from non-lesion ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = train_ds[700]\n",
    "\n",
    "batched_x = batched_data.x[:, :3]\n",
    "ffr_feats = batched_data.x[:,3:]\n",
    "ffr_feats[ffr_feats != 0.] = 1 \n",
    "out = model.gnn(batched_x, batched_data.edge_index).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "    init='random', perplexity=3).fit_transform(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:,1], c=ffr_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:,1], c=ffr_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:,1], c=ffr_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:,1], c=ffr_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from cardio.dataset import Fame2GraphDatasetWrapper\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHECKPOINT_PATH_GNN = \"checkpoint/diameterStenosisClf.ckpt\"\n",
    "\n",
    "class Conf:\n",
    "    gradient_clipping = None\n",
    "    nb_epochs = 5\n",
    "    eval_every_epochs = 1\n",
    "    log_every_steps = 50\n",
    "    val_batch_size = 2 \n",
    "    train_batch_size = 2 \n",
    "    test_batch_size = 2\n",
    "    num_workers = 1\n",
    "    checkpoint = False\n",
    "    device = \"cpu\"\n",
    "    model = \"gnn\"\n",
    "    run_mode = \"simple\"\n",
    "    task = \"forecast\"\n",
    "    dataset_dir = \"data/datasets/eventForecastingDelaunayWithClinicalData\"\n",
    "    \n",
    "    # Model configs\n",
    "    gnn_norm = \"BatchNorm\"\n",
    "    gnn_pooling_cls_name = \"CustomPooling\"  \n",
    "    weight_init_strategy = \"\"\n",
    "    gnn_cls_name = \"GIN\"\n",
    "    gnn_nb_layers = 5\n",
    "    gnn_dropout = 0.5973039417606918     \n",
    "    gnn_node_emb_dim = 256\n",
    "    gnn_global_hidden_dim = 512\n",
    "    \n",
    "    gnn_ffr_pooling_proj_dim = 16\n",
    "    gnn_node_features = 4\n",
    "    gnn_ffr_pooling_factor = 2\n",
    "\n",
    "    gnn_act = 'relu'\n",
    "    nb_classes = 1\n",
    "    weight_init_strategy = \"Xavier Uniform\"\n",
    "    init_weights_from = None\n",
    "    gnn_freeze_weights = False\n",
    "    loss_func=\"FocalLoss\"\n",
    "    checkpoint_after_n_epochs = 5\n",
    "\n",
    "    init_weights_for = 'both'\n",
    "    init_weights_from= CHECKPOINT_PATH_GNN\n",
    "    use_balanced_batches = True\n",
    "    \n",
    "confs = Conf()\n",
    "\n",
    "''' Custom kflod fit using pyotrchlighting that returns validation metrics'''\n",
    "folds = 5\n",
    "confs.checkpoint = False # avoit checkpoints on kfold so we dont explode disk\n",
    "\n",
    "# Resolvers\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "# train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.train_dataset_dir)\n",
    "# test_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.test_dataset_dir )\n",
    "dm = resolvers.datamodule_resolver(confs)\n",
    "model = resolvers.model_resolver(confs, None, None, None).model\n",
    "model.eval()\n",
    "\n",
    "dm.setup(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Go from graphs to patients\n",
    "pat_id = pd.DataFrame(data=[(g.patient_id, g.y.item()) for g in train_ds], columns=[\"patient\", \"voce\"])\n",
    "grouped_patients = pat_id.groupby(\"patient\").mean() > 0.1\n",
    "y = grouped_patients['voce'].values\n",
    "pat_ids = grouped_patients.index.values\n",
    "\n",
    "train_pat_ids, val_patient_ids = train_test_split(np.arange(len(y)), test_size=0.1, stratify=y)\n",
    "\n",
    "# Go back to patients from graphs\n",
    "graph_ds_indexes_train = [i for i, g in enumerate(train_ds) if g.patient_id in pat_ids[train_pat_ids]] \n",
    "graph_ds_indexes_val = [i for i, g in enumerate(train_ds) if g.patient_id in pat_ids[val_patient_ids]] \n",
    "len(graph_ds_indexes_train), len(graph_ds_indexes_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.index_select(graph_ds_indexes_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(grouped_patients.iloc[x2]['voce'].values.astype(np.int32)))\n",
    "print(Counter(grouped_patients.iloc[x]['voce'].values.astype(np.int32)))\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = np.zeros((len(train_ds), 1), dtype=np.float32)\n",
    "y_train = np.zeros((len(train_ds), 1), dtype=np.int32)\n",
    "\n",
    "pred_test = np.zeros((len(test_ds), 1), dtype=np.float32)\n",
    "y_test = np.zeros((len(test_ds), 1), dtype=np.int32)\n",
    "\n",
    "# Create Train \n",
    "for i, graph in tqdm(enumerate(train_ds), desc=\"train data\", total=len(train_ds)):\n",
    "    # Prepare data\n",
    "    graph.batch = torch.zeros(len(graph.x), dtype=torch.int64)\n",
    "\n",
    "    y_train[i] = graph.y.item()\n",
    "    pred_train[i] = model(graph).detach().numpy()\n",
    "\n",
    "# Create Test\n",
    "for i, graph in tqdm(enumerate(test_ds), desc=\"test data\", total=len(test_ds)):\n",
    "    \n",
    "    # Prepare data\n",
    "    graph.batch = torch.zeros(len(graph.x), dtype=torch.int64)\n",
    "\n",
    "    y_test[i] = graph.y.item()\n",
    "    pred_test[i] = model(graph).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2)\n",
    "sig_pred_train = 1/(1 + np.exp(-pred_train.reshape(-1)))\n",
    "sns.histplot(x=sig_pred_train > 0.5, ax=ax[0])\n",
    "sns.histplot(x=y_train.reshape(-1), ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "sig_pred_test = 1/(1 + np.exp(-pred_test.reshape(-1)))\n",
    "cm = confusion_matrix(y_test, sig_pred_test > 0.5)\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/fame2_clinical_events_2year_data.csv\")\n",
    "test_pats = pd.read_csv(\"data/datasets/test_split.csv\", header=None)\n",
    "l = [f[0] for f in test_pats.values]\n",
    "\n",
    "df_test = df[df.Patient.isin(l)]\n",
    "df_test.DS.hist()\n",
    "plt.axvline(x=50, c=\"r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centerline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    duplicate_imgs_with_multiple_lesions=False, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup(split_path=SPLIT_PATH,\n",
    "    split_type= SplitType.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for a, d in fame2_ds.df.groupby(['base_path', 'artery', 'lesion_name']):\n",
    "    l.append( d.has_lesion.values[0] )\n",
    "\n",
    "Counter(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.has_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import skeletonize\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Invert the horse image\n",
    "# image = invert(data.horse())\n",
    "image = fame2_ds[0][0][1,:].numpy()\n",
    "# perform skeletonization\n",
    "\n",
    "# may need to close holes\n",
    "kernelSize = (3,3)  \n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "image_filled = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "skeleton = skeletonize(image_filled)\n",
    "\n",
    "# display results\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('original', fontsize=20)\n",
    "\n",
    "ax[1].imshow(skeleton, cmap=plt.cm.gray)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('skeleton', fontsize=20)\n",
    "\n",
    "ax[2].imshow(image + skeleton , cmap=plt.cm.gray)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('image + skeleton', fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find % of same images we classify differently because they are different views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    duplicate_imgs_with_multiple_lesions=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup(split_path=SPLIT_PATH,\n",
    "    split_type= SplitType.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "raw_stacked, lesion_mask, df_info = fame2_ds[0]\n",
    "\n",
    "raw_img = raw_stacked[0].numpy().astype(np.uint8)\n",
    "artery_mask = raw_stacked[1].numpy().astype(np.uint8)\n",
    "lesion_mask = lesion_mask.numpy().astype(np.uint8)\n",
    "\n",
    "edge_mask = cv2.Canny(artery_mask, 1, 2)\n",
    "coordinates = np.vstack(np.where(edge_mask != 0)).transpose(1,0)\n",
    "assert len(np.unique(lesion_mask)) == 3 and df_info.shape[0] == 1\n",
    "lesion_points = (lesion_mask[coordinates[:,0], coordinates[:,1]] == 2).astype(np.float32).reshape(-1,1)\n",
    "\n",
    "attach_features = ['FFR']\n",
    "feature_transforms = [ None ]\n",
    "feature_tensors = []\n",
    "for i in range(len(attach_features)):\n",
    "    feature = attach_features[i]\n",
    "    transform = feature_transforms[i]\n",
    "    if transform != None:\n",
    "        value = transform(df_info[feature].values[0])\n",
    "    else:\n",
    "        value = df_info[feature].values[0]\n",
    "    assert not pd.isna(value), f\"Could not attach value to data point, due to null feature `{feature}`\"\n",
    "\n",
    "    # An array where we have only N elements (bases on teh edge_mask coordiantes) and the values are \n",
    "    # 0 if not a lesion and 1 if a lesion\n",
    "    feature = (lesion_mask[coordinates[:,0], coordinates[:,1]] == 2).astype(np.float32).reshape(-1,1)\n",
    "    # Then use the lesion only points of the edge and assign them the value\n",
    "    feature[feature == 1.0] = value\n",
    "    feature_tensors.append(feature)\n",
    "\n",
    "# Skip the pixel intensity\n",
    "features = np.hstack([raw_img[coordinates[:,0], coordinates[:,1]].reshape(-1,1), coordinates] + feature_tensors)\n",
    "\n",
    "coordinates.shape, features.shape, edge_mask.shape, lesion_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "from cardio.dataset import  Fame2GraphDatasetWrapper\n",
    "\n",
    "CHECKPOINT_PATH_GNN = \"checkpoint/GIN-L1-F3-E128_LesionPooling-G512_E20_EF_DE.ckpt\"\n",
    "\n",
    "class Conf:\n",
    "    loss_func = \"BCEWithLogitsLoss\"\n",
    "    task = \"forecast\"\n",
    "    gradient_clipping = None\n",
    "    nb_epochs = 5\n",
    "    eval_every_epochs = 1\n",
    "    log_every_steps = 50\n",
    "    val_batch_size = 1 \n",
    "    train_batch_size = 1 \n",
    "    test_batch_size = 1 \n",
    "    num_workers = 4\n",
    "    checkpoint = False\n",
    "    device = \"cpu\"\n",
    "    model = \"gnn\"\n",
    "    run_mode = \"simple\"\n",
    "    train_dataset_dir = \"/Users/tbelmpas/Code/EPFL/thesis/TheoBelmpas-2022-mas-MiFromFAME2-AI4CARDIO/data/datasets/eventForecastingDelaunayNoPixelsOnlyLesions/train\"\n",
    "    test_dataset_dir = \"/Users/tbelmpas/Code/EPFL/thesis/TheoBelmpas-2022-mas-MiFromFAME2-AI4CARDIO/data/datasets/eventForecastingDelaunayNoPixelsOnlyLesions/test\"\n",
    "\n",
    "    # Model configs\n",
    "    gnn_norm = \"BatchNorm\"\n",
    "    gnn_pooling_cls_name = \"LesionPooling\"\n",
    "    weight_init_strategy = \"\"\n",
    "    gnn_cls_name = \"GIN\"\n",
    "    gnn_nb_layers = 1\n",
    "    gnn_dropout = 0.5973039417606918     \n",
    "    gnn_node_emb_dim = 128\n",
    "    gnn_global_hidden_dim = 512\n",
    "    gnn_ffr_pooling_proj_dim = 16\n",
    "    gnn_node_features = 3\n",
    "    gnn_ffr_pooling_factor = 2\n",
    "    gnn_act = 'relu'\n",
    "    nb_classes = 1\n",
    "    weight_init_strategy = \"Xavier Uniform\"\n",
    "    init_weights_from = None\n",
    "    gnn_freeze_weights = False\n",
    "\n",
    "    init_weights_for = 'both'\n",
    "    init_weights_from= CHECKPOINT_PATH_GNN\n",
    "    use_balanced_batches = False\n",
    "    \n",
    "import os\n",
    "confs = Conf()\n",
    "if not os.path.isdir(confs.train_dataset_dir):\n",
    "    raise ValueError(\"Train path doesnt exist\")\n",
    "\n",
    "''' Custom kflod fit using pyotrchlighting that returns validation metrics'''\n",
    "folds = 5\n",
    "confs.checkpoint = False # avoit checkpoints on kfold so we dont explode disk\n",
    "\n",
    "# Resolvers\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.train_dataset_dir)\n",
    "test_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=confs.test_dataset_dir )\n",
    "model = resolvers.model_resolver(confs, None).model\n",
    "model.eval()\n",
    "\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "y_train = np.zeros((len(train_ds), 1), dtype=np.int32)\n",
    "pred_train = np.zeros((len(train_ds), 1), dtype=np.float32)\n",
    "ffr_train = np.zeros((len(train_ds), 1), dtype=np.float32)\n",
    "train_lesions_id = [None] * len(train_ds)\n",
    "\n",
    "y_test = np.zeros((len(test_ds), 1), dtype=np.int32)\n",
    "pred_test = np.zeros((len(test_ds), 1), dtype=np.float32)\n",
    "ffr_test = np.zeros((len(test_ds), 1), dtype=np.float32)\n",
    "test_lesions_id = [None] * len(test_ds)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def model_predict(model, input):\n",
    "    return torch.sigmoid(model(input)).item()\n",
    "\n",
    "# Create Train \n",
    "fame2_ds.setup(\"data/datasets/\", SplitType.train)\n",
    "for i, graph in tqdm(enumerate(train_ds), total=len(train_ds)):\n",
    "    # Prepare data\n",
    "    ds_items = fame2_ds[i]\n",
    "    graph.batch = torch.zeros(len(graph.x), dtype=torch.int64)\n",
    "    ffr = graph.x[:,3][graph.x[:,3] != 0.0][1].item() # FFR is in the last column of x, but only exists for lesion nodes\n",
    "    lesion_id = ds_items[2].apply(\n",
    "            lambda x: f\"{x['patient_id_x']}_{x['lesion_name'].replace('lesion', '') if x['lesion_name'] else 'None'}\",\n",
    "            axis =1).values[0]\n",
    "\n",
    "    if (not np.isclose(ffr, ds_items[2].FFR.values[0])):\n",
    "        print( f\"Found diff ffrs in line {i} -> {ffr:.2f} vs {ds_items[2].FFR.values[0]:.2f}\" )\n",
    "        raise RuntimeError(\"bra\")\n",
    "\n",
    "    y_train[i] = graph.y.item()\n",
    "    pred_train[i] = model_predict(model, graph)\n",
    "    ffr_train[i,0] = ffr\n",
    "    train_lesions_id[i] = lesion_id \n",
    "\n",
    "# Create Test\n",
    "fame2_ds.setup(\"data/datasets/\", SplitType.test)\n",
    "for i, graph in tqdm(enumerate(test_ds), total=len(test_ds)):\n",
    "    \n",
    "    # Prepare data\n",
    "    ds_items = fame2_ds[i]\n",
    "    graph.batch = torch.zeros(len(graph.x), dtype=torch.int64)\n",
    "    ffr = graph.x[:,3][graph.x[:,3] != 0.0][1].item() # FFR is in the last column of x, but only exists for lesion nodes\n",
    "    lesion_id = ds_items[2].apply(\n",
    "            lambda x: f\"{x['patient_id_x']}_{x['lesion_name'].replace('lesion', '') if x['lesion_name'] else 'None'}\",\n",
    "            axis =1).values[0]\n",
    "\n",
    "    assert np.isclose(ffr, ds_items[2].FFR.values[0]), f\"Found diff ffrs in line {i}\"\n",
    "\n",
    "    y_test[i] = graph.y.item()\n",
    "    pred_test[i] = model_predict(model, graph)\n",
    "    ffr_test[i,0] = ffr\n",
    "    test_lesions_id[i] = lesion_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(data={\n",
    "    'lesion_id': train_lesions_id, 'pred': pred_train.reshape(-1), \n",
    "    'label': y_train.reshape(-1), 'ffr': ffr_train.reshape(-1) \n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame(data={\n",
    "    'lesion_id': test_lesions_id, 'pred': pred_test.reshape(-1), \n",
    "    'label': y_test.reshape(-1), 'ffr': ffr_test.reshape(-1) \n",
    "})\n",
    "\n",
    "len(train_data.groupby('lesion_id').count())/len(train_data), len(test_data.groupby('lesion_id').count())/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_confident(gdf):\n",
    "    preds = gdf.pred.values - 0.5\n",
    "    preds[preds <0.0] = preds[preds <0.0] * -1\n",
    "    return pd.Series(data=[gdf.pred.values[np.argmax(preds)], gdf.label.values[0]], index=[\"pred\", \"label\"])\n",
    "most_conf_test_data = test_data.groupby('lesion_id').apply(most_confident)\n",
    "most_conf_test_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "agg_test_data = test_data.groupby('lesion_id').agg(\n",
    "    pred_mean=('pred', 'mean'), stdp=('pred', 'std'), \n",
    "    pred_min=('pred', 'min'), label=('label', 'min'),\n",
    "    pred_max=('pred', 'max'))\n",
    "\n",
    "def most_confident(gdf):\n",
    "    preds = gdf.pred.values - 0.5\n",
    "    preds[preds <0.0] = preds[preds <0.0] * -1\n",
    "    return pd.Series(data=[gdf.pred.values[np.argmax(preds)], gdf.label.values[0]], index=[\"pred\", \"label\"])\n",
    "most_conf_test_data = test_data.groupby('lesion_id').apply(most_confident)\n",
    "\n",
    "f, ax = plt.subplots(1,2)\n",
    "agg_test_data.pred_mean.hist(ax=ax[0])\n",
    "agg_test_data.stdp.hist(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(gdf):\n",
    "    \n",
    "    return \n",
    "test_data.groupby('lesion_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "metrics_no_agg = [\n",
    "    accuracy_score(test_data.label.values, test_data.pred.values > 0.5), \n",
    "    recall_score(test_data.label.values, test_data.pred.values > 0.5),\n",
    "    precision_score(test_data.label.values, test_data.pred.values > 0.5),\n",
    "    f1_score(test_data.label.values, test_data.pred.values > 0.5),\n",
    "    roc_auc_score(test_data.label.values, test_data.pred.values > 0.5)]\n",
    "\n",
    "metrics_min = [\n",
    "    accuracy_score(agg_test_data.label.values, agg_test_data.pred_min.values > 0.5), \n",
    "    recall_score(agg_test_data.label.values, agg_test_data.pred_min.values > 0.5),\n",
    "    precision_score(agg_test_data.label.values, agg_test_data.pred_min.values > 0.5),\n",
    "    f1_score(agg_test_data.label.values, agg_test_data.pred_min.values > 0.5),\n",
    "    roc_auc_score(agg_test_data.label.values, agg_test_data.pred_min.values > 0.5)]\n",
    "\n",
    "metrics_max = [\n",
    "    accuracy_score(agg_test_data.label.values, agg_test_data.pred_max.values > 0.5), \n",
    "    recall_score(agg_test_data.label.values, agg_test_data.pred_max.values > 0.5),\n",
    "    precision_score(agg_test_data.label.values, agg_test_data.pred_max.values > 0.5),\n",
    "    f1_score(agg_test_data.label.values, agg_test_data.pred_max.values > 0.5),\n",
    "    roc_auc_score(agg_test_data.label.values, agg_test_data.pred_max.values > 0.5)]\n",
    "\n",
    "metrics_most_confident = [\n",
    "    accuracy_score(most_conf_test_data.label.values, most_conf_test_data.pred.values > 0.5), \n",
    "    recall_score(most_conf_test_data.label.values, most_conf_test_data.pred.values > 0.5),\n",
    "    precision_score(most_conf_test_data.label.values, most_conf_test_data.pred.values > 0.5),\n",
    "    f1_score(most_conf_test_data.label.values, most_conf_test_data.pred.values > 0.5),\n",
    "    roc_auc_score(most_conf_test_data.label.values, most_conf_test_data.pred.values > 0.5)]\n",
    "\n",
    "metrics_df = pd.DataFrame(data={'no_agg': metrics_no_agg ,'use_min': metrics_min, 'use_max': metrics_max, 'use_most_conf': metrics_most_confident}, index=['Acc', 'Rcl', 'Prc', 'F1', 'Auc'])\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import generate_fame2_patches_ds\n",
    "\n",
    "generate_fame2_patches_ds(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    EVENT_COLUMNS, \n",
    "    \"data/datasets/baselinePatches/\",\n",
    "    \"data/datasets/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.pylightning import Fame2Baseline\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456], std=[0.229, 0.224])\n",
    "])\n",
    "\n",
    "class Conf:\n",
    "    dataset_dir:str\n",
    "    num_workers = 4\n",
    "    train_batch_size = 2\n",
    "confs = Conf()\n",
    "confs.dataset_dir = \"data/datasets/baselinePatches\"\n",
    "dm = Fame2Baseline(confs, preprocess)\n",
    "dm.setup(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.networks.baseline as bs\n",
    "\n",
    "dl = dm.train_dataloader()\n",
    "i = next(iter(dl))\n",
    "model = bs.BaselineResNet(0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Node removes to make graph denser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.preprocess.graph import *\n",
    "from skimage.morphology import skeletonize\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    \n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    duplicate_imgs_with_multiple_lesions=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()\n",
    "\n",
    "\n",
    "pipeline = ImageToGraphPipeline(\n",
    "    LesionOnlyPixelExtractor(True, ['FFR', 'exactSegmentLocation'], [None, artery_segment_to_float]), \n",
    "    PixelSampler(), \n",
    "    DelaunayVertexConnector(10),\n",
    "    TASK_TO_LABEL_EXTRACTOR['EventForecast'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.df.DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pipeline = ImageToGraphPipeline(\n",
    "    LesionOnlyPixelExtractor(True, ['FFR', 'exactSegmentLocation'], [None, artery_segment_to_float]), \n",
    "    NoSampler(), \n",
    "    DelaunayVertexConnector(10),\n",
    "    TASK_TO_LABEL_EXTRACTOR['EventForecast'],\n",
    "    1, 1\n",
    ")\n",
    "f, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "ds_item = fame2_ds[834]\n",
    "\n",
    "coordinates, features, lesion_points = pipeline.generate_sampled_vertices(ds_item)\n",
    "edge_index = pipeline.vertex_connector.connect(coordinates, features)\n",
    "print(features.shape)\n",
    "find_neighs_of = 5\n",
    "al = torch.where( edge_index[:,0] == find_neighs_of )[0]\n",
    "ar = torch.where( edge_index[:,1] == find_neighs_of )[0]\n",
    "a = list(set( edge_index[al][:,1].numpy().tolist() + edge_index[ar][:,0].numpy().tolist()))\n",
    "neighs = coordinates[a]\n",
    "\n",
    "\n",
    "ax[0].scatter(coordinates[:,0], coordinates[:,1], s=1)\n",
    "ax[0].scatter(neighs[:,0], neighs[:,1], s=6)\n",
    "\n",
    "\n",
    "pipeline = ImageToGraphPipeline(\n",
    "    LesionOnlyPixelExtractor(True, ['FFR', 'exactSegmentLocation'], [None, artery_segment_to_float]), \n",
    "    StochasticDownsampler(), \n",
    "    DelaunayVertexConnector(10),\n",
    "    TASK_TO_LABEL_EXTRACTOR['EventForecast'],\n",
    "    0.25,\n",
    "    1\n",
    ")\n",
    "coordinates, features, lesion_points = pipeline.generate_sampled_vertices(ds_item)\n",
    "edge_index = pipeline.vertex_connector.connect(coordinates, features)\n",
    "print(features.shape)\n",
    "al = torch.where( edge_index[:,0] == find_neighs_of )[0]\n",
    "ar = torch.where( edge_index[:,1] == find_neighs_of )[0]\n",
    "a = list(set( edge_index[al][:,1].numpy().tolist() + edge_index[ar][:,0].numpy().tolist()))\n",
    "neighs = coordinates[a]\n",
    "\n",
    "ax[1].scatter(coordinates[:,0], coordinates[:,1], s=1)\n",
    "ax[1].scatter(neighs[:,0], neighs[:,1], s=7)\n",
    "\n",
    "print(\"HERE\", pipeline.point_label_extractor_callback(ds_item[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds_item[0][1].numpy().astype(np.uint8)\n",
    "kernelSize = (4,4)  \n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "image_filled = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "skeleton = skeletonize(image_filled)\n",
    "skeleton_coordinates_x = np.where(skeleton)[0]\n",
    "skeleton_coordinates_y = np.where(skeleton)[1]\n",
    "skeleton_coordinates = np.vstack([skeleton_coordinates_x, skeleton_coordinates_y])\n",
    "\n",
    "plt.imshow(skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distances = np.zeros((len(coordinates),1))\n",
    "for i, point in enumerate(coordinates.numpy()):\n",
    "    b = skeleton_coordinates - point.reshape(2,1)\n",
    "    min_distances[i] = np.min(np.linalg.norm(b,axis=0))\n",
    "\n",
    "min_distances.mean(), min_distances.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(min_distances.shape[0]), min_distances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count data points with specific attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* count data points with ffr in between 0.7 and 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=True,\n",
    "    duplicate_imgs_with_multiple_lesions=False,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup(SPLIT_PATH, SplitType.train)\n",
    "train_patients, test_patients = load_train_test_split(SPLIT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fame2_ds.df\n",
    "# group_columns =  ['patient_id_x', 'artery']\n",
    "group_columns = ['base_path', 'artery']\n",
    "\n",
    "l = list(df[(df.has_label) & (df.has_lesion)]\n",
    "                    # .groupby(group_columns).filter(lambda x: len(x) == 2) # keep only 2 views\n",
    "                    .groupby(group_columns))\n",
    "len(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find differences in predictions between: FFR, DS and GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=True,\n",
    "    duplicate_imgs_with_multiple_lesions=False,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "class Conf:\n",
    "    gradient_clipping = None\n",
    "    nb_epochs = 5\n",
    "    eval_every_epochs = 1\n",
    "    log_every_steps = 50\n",
    "    val_batch_size = 8\n",
    "    train_batch_size = 8\n",
    "    num_workers = 4\n",
    "    task = \"forecast\"\n",
    "    checkpoint = False\n",
    "    device = \"cpu\"\n",
    "    model = \"gnn\"\n",
    "    run_mode = \"simple\"\n",
    "    dataset_dir = \"data/datasets/eventForecastDelaunayOnlyLesions\"\n",
    "confs = Conf()\n",
    "confs.checkpoint = False # avoit checkpoints on kfold so we dont explode disk\n",
    "\n",
    "# Resolvers\n",
    "datamodule = resolvers.datamodule_resolver(confs)\n",
    "\n",
    "# setup\n",
    "datamodule.setup(None)\n",
    "\n",
    "# get the dataframe with clinical data\n",
    "df = fame2_ds.df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predict FFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification as metrics\n",
    "import torchmetrics as tmetrics\n",
    "\n",
    "# \n",
    "ffr_threshold = 0.8\n",
    "ds_threshold = 70\n",
    "test_ffr, train_ffr = [], []\n",
    "test_ds, train_ds = [], []\n",
    "test_y, train_y = [], []\n",
    "for type, ds in [(\"test\", datamodule.test_dataset), (\"train\", datamodule.train_dataset)]:\n",
    "    for graph in ds:\n",
    "        ffr = df[(df.patient_id_x == graph.patient_id) & (df.lesion_id_syntax == graph.lesion_id)].FFR.values[0]\n",
    "        ds = df[(df.patient_id_x == graph.patient_id) & (df.lesion_id_syntax == graph.lesion_id)].DS.values[0]\n",
    "        assert not pd.isna(ds) and not pd.isna(ffr)\n",
    "        \n",
    "        if type == 'test':\n",
    "            test_ffr.append(ffr)\n",
    "            test_ds.append(ds)\n",
    "            test_y.append(graph.y.item())\n",
    "        else:\n",
    "            train_ffr.append(ffr)\n",
    "            train_ds.append(ds)\n",
    "            train_y.append(graph.y.item())\n",
    "\n",
    "test_ffr = np.array(test_ffr)\n",
    "test_ds = np.array(test_ds)\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "test_pred_ffr = torch.tensor([1.0 if ffr < ffr_threshold else 0.0 for ffr in test_ffr], dtype=torch.float)\n",
    "test_pred_ds  = torch.tensor([1.0 if ds >= ds_threshold else 0.0 for ds in test_ds], dtype=torch.float)\n",
    "\n",
    "\n",
    "train_ffr = np.array(train_ffr)\n",
    "train_ds = np.array(train_ds)\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "train_pred_ffr = torch.tensor([1.0 if ffr < ffr_threshold else 0.0 for ffr in train_ffr], dtype=torch.float)\n",
    "train_pred_ds  = torch.tensor([1.0 if ds >= ds_threshold else 0.0 for ds in train_ds], dtype=torch.float)\n",
    "\n",
    "test_metrics = [metrics.BinaryAccuracy(), metrics.BinaryPrecision(), metrics.BinaryRecall(), metrics.BinaryF1Score(), tmetrics.AUROC(2,1), metrics.Specificity(), tmetrics.ConfusionMatrix(2)]\n",
    "metrics_store = { f\"{m.__class__.__name__}\" if m.__class__.__name__ != \"ConfusionMatrix\" else \"tn, fp, fn,tp\" :list()  for m in test_metrics }\n",
    "metrics_store['type'] = list()\n",
    "metrics_store['split'] = list()\n",
    "for type, split, pred in [('ffr', 'test', test_pred_ffr), ('ds','test', test_pred_ds), ('ffr', 'train', train_pred_ffr), ('ds','train', train_pred_ds)]:\n",
    "    y_true = test_y if split == 'test' else train_y\n",
    "    for m in test_metrics:\n",
    "        if m.__class__.__name__ == \"ConfusionMatrix\":\n",
    "            cm  = m(pred, y_true)\n",
    "            metrics_store[\"tn, fp, fn,tp\"].append( cm.numpy().flatten() )\n",
    "        else:\n",
    "            metrics_store[f\"{m.__class__.__name__}\"].append(m(pred, y_true).item())  \n",
    "        m.reset()\n",
    "    metrics_store['type'].append(type)\n",
    "    metrics_store['split'].append(split)\n",
    "\n",
    "scores_ffr_ds = pd.DataFrame(data=metrics_store).set_index([\"split\", \"type\"])\n",
    "\n",
    "scores_ffr_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the ffr and ds distr on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "ax[0][0].set_title(\"DS hist test\")\n",
    "ax[0][0].hist(test_ds)\n",
    "ax[0][1].set_title(\"FFR hist test\")\n",
    "ax[0][1].hist(test_ffr, bins=30)\n",
    "\n",
    "ax[1][0].set_title(\"DS hist train\")\n",
    "ax[1][0].hist(train_ds)\n",
    "ax[1][1].set_title(\"FFR hist train\")\n",
    "ax[1][1].hist(train_ffr, bins=30)\n",
    "f.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check if predictions of FFR and DS overlap in terms of fp and fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are FFR and DS predictions for events overlapping \n",
    "overlap_ffr_ds = torch.where( (test_pred_ds == 1) & (test_pred_ffr == 1) )[0]\n",
    "overlap_ffr_ds_to_real = torch.where( test_y[overlap_ffr_ds] == 1)[0]\n",
    "assert overlap_ffr_ds_to_real.shape[0] == 23\n",
    "\n",
    "# Are the 14 fp of ds in the fp set of ffr also\n",
    "fp_ds = torch.where( (test_pred_ds == 1) & (test_y == 0) )[0]\n",
    "overlap_fp_ds_w_fp_ffr = torch.where( test_pred_ffr[fp_ds] == 1 )[0]\n",
    "print(overlap_fp_ds_w_fp_ffr.shape[0], fp_ds.shape[0])\n",
    "\n",
    "# ---> Yes FFR predicted events on every point DS predicted events too "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check what are the value distributions of fn for both FFR and DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fn = torch.where((test_pred_ds == 0) & (test_y == 1))[0].numpy()\n",
    "ffr_fn = torch.where((test_pred_ffr == 0) & (test_y == 1))[0].numpy()\n",
    "\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].set_title(\"DS false positive value distribution\")\n",
    "ax[0].hist(test_ds[ds_fn])\n",
    "ax[1].set_title(\"FFR false positive value distribution\")\n",
    "ax[1].hist(test_ffr[ffr_fn])\n",
    "f.tight_layout()\n",
    "print(f\"Len fn ds: {ds_fn.shape[0]} | Len fn ffr: {ffr_fn.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Docs said that all \"unknown\" ffr's take a default 0.5 value, let's see how many we miss with values == 0.5\n",
    "\n",
    "**Answer:** This doesn't affect our predictions because only 10/111 false positives have a value of 0.5\n",
    "\n",
    "**hint**: We can remedy the fp of FFR by testing if the DS is high enough\n",
    "* To do that, we cannot use th DS, because the points in-between 0.7 and 0.8 (which case the fp to be high) have a unifrom DS around 0.5 so no split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_ffr = torch.where((test_pred_ffr == 1) & (test_y == 0))[0]\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=(15,3))\n",
    "ax[0].hist(test_ffr[fp_ffr], bins=30)\n",
    "ax[0].set_title(\"FFR distribution false pos\")\n",
    "ax[1].hist(test_ds[fp_ffr], bins=20)\n",
    "ax[1].set_title(\"DS distribution for FFR false pos preds\")\n",
    "ax[2].hist(test_ds[np.where((test_ffr[fp_ffr] >=0.7) & (test_ffr[fp_ffr] <0.8))[0]], bins=20)\n",
    "ax[2].set_title(\"DS distribution for FFR false pos preds focus on FFR \\in [0.7, 0.8)\")\n",
    "\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_ffr = torch.where((test_pred_ffr == 1) & (test_y == 1))[0]\n",
    "fn_ds = torch.where((test_pred_ds == 0) & (test_y == 1))[0]\n",
    "\n",
    "mask = np.intersect1d(tp_ffr, fn_ds)\n",
    "\n",
    "f, ax = plt.subplots(1,2, figsize=(10,3))\n",
    "ax[0].hist(test_ffr[mask])\n",
    "ax[0].set_title(\"FFR true positive distribution (intersect DS fn)\")\n",
    "ax[1].hist(test_ds[mask], bins=20)\n",
    "ax[1].set_title(\"FFR true positive's DS distribution (intersect DS fn)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "sns.histplot(x=df[df.has_lesion].DM_Insulin.values, hue=df[df.has_lesion].VOCE.values, ax=ax[0][0])\n",
    "ax[0][0].set_title(\"Insulin threshold (hue: VOCE)\")\n",
    "\n",
    "sns.histplot(x=df[df.has_lesion].Prev_MI.values, hue=df[df.has_lesion].VOCE.values, ax=ax[0][1])\n",
    "ax[0][1].set_title(\"Prev_MI (hue: VOCE)\")\n",
    "\n",
    "sns.histplot(x=df[df.has_lesion].Lesion_Length.values, hue=df[df.has_lesion].VOCE.values, ax=ax[1][0])\n",
    "ax[1][0].set_title(\"Lesion_Length (hue: VOCE)\")\n",
    "ax[1][0].set_xlim(0,40)\n",
    "\n",
    "sns.histplot(x=df[df.has_lesion].Smoker.values, hue=df[df.has_lesion].VOCE.values, ax=ax[1][1])\n",
    "ax[1][1].set_title(\"Smoker (hue: VOCE)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on data\n",
    "\n",
    "* standard scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from cardio.dataset.fame2_raw_data_loader import Fame2RawDSLoader, load_train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/fame2_clinical_events_2year_data.csv\")\n",
    "df.head()\n",
    "\n",
    "CLINICAL_DATA_FEATURES = ['FFR', 'DS', 'HTN', 'Hchol', 'DM_Overall',\n",
    "       'DM_Insulin', 'Ren_Ins', 'PVD', 'CVA', 'Prev_MI', 'Prev_PCI', 'CCS',\n",
    "       'CCS_3', 'Silent_Ischemia', 'LVEF', 'MVD', 'Lesion_Type', 'Age', 'Male',\n",
    "       'BMI', 'CAD', 'Smoker', 'Age_64', 'BMI_28']\n",
    "\n",
    "train_pids, test_pids = load_train_test_split(\"./data/datasets/simpleSplit\")\n",
    "train_df = df[(df.Patient.isin(train_pids))]\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(train_df[CLINICAL_DATA_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[CLINICAL_DATA_FEATURES] = scaler.transform(df[CLINICAL_DATA_FEATURES])\n",
    "\n",
    "df.to_csv(\"data/fame2_clinical_events_2year_data_std_scaled.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add radiomix slices in GNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=False,\n",
    "    duplicate_imgs_with_multiple_lesions=False,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiomics import firstorder, shape2D, glcm, featureextractor\n",
    "import SimpleITK as sitk\n",
    "import cv2 \n",
    "from skimage.morphology import skeletonize\n",
    "import logging\n",
    "\n",
    "# set level for all classes\n",
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "def gen_radius_mask_2d(array, i,j,radius, shape_mask=None):\n",
    "    ''' Creates a 2d mask like array, with 0 everywhere except (i,j) and a radius around it which has 1\n",
    "        if shape_mask != None then do a bitwise and between the radius mask and the lesion mask\n",
    "    '''\n",
    "    mask = np.zeros_like(array)\n",
    "    mask[i,j] = 1\n",
    "    d1l = i-radius if i-radius >= 0 else 0 \n",
    "    d2l = j-radius if j-radius >= 0 else 0 \n",
    "    mask[d1l:i+(radius+1), d2l:j+(radius+1)] = 1\n",
    "\n",
    "    if isinstance(shape_mask, np.ndarray):\n",
    "        return np.bitwise_and(mask, shape_mask)\n",
    "    else:\n",
    "        return mask\n",
    "\n",
    "def generate_radiomix_per_coordinate(radiomix_extractor, nb_features, coordinates, raw_img, artery_mask, radius=5):\n",
    "    ''' Generate radiomix features for each coordinate pair\n",
    "\n",
    "    '''\n",
    "    feature_values = np.zeros((len(coordinates), nb_features), dtype=np.float32)\n",
    "    features = None\n",
    "    for idx, (i,j) in enumerate(coordinates):\n",
    "        ma = gen_radius_mask_2d(raw_img, i, j, radius, artery_mask)\n",
    "\n",
    "        im = sitk.GetImageFromArray(raw_img)\n",
    "        ma = sitk.GetImageFromArray(ma)\n",
    "\n",
    "        featureVector = radiomix_extractor.execute(im, ma)\n",
    "        if features == None:\n",
    "            features = [k[9:] for k in featureVector.keys() if 'original' == k[:8] ]\n",
    "        feature_values[idx,:] = np.array([featureVector['original_'+k] for k in features])\n",
    "\n",
    "    return features, feature_values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "radiomix_categories = ['glcm', 'glrlm', 'shape2D']\n",
    "radiomix_shape2D_features = ['MeshSurface', 'Perimeter', 'MaximumDiameter', 'MajorAxisLength', 'MinorAxisLength', 'Elongation']\n",
    "\n",
    "# The features generated by this categories are 46, please dont change anything\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "extractor.disableAllFeatures()\n",
    "extractor.enableFeaturesByName(shape2D=radiomix_shape2D_features)\n",
    "extractor.enableFeatureClassByName('glcm')\n",
    "extractor.enableFeatureClassByName('glrlm')\n",
    "\n",
    "# Stores all data in a list with format: (base_path, patient_id, lesions_id_syntax, radiomix_features)\n",
    "data = [ ]\n",
    "for ds_item in tqdm(fame2_ds):\n",
    "    # Other features\n",
    "    base_path  = ds_item[2].base_path.values[0]\n",
    "    patient_id = ds_item[2].patient_id_x.values[0]\n",
    "    lesion_id_syntax =  ds_item[2].lesion_id_syntax.values[0]\n",
    "    VOCE =  ds_item[2].Event.values[0]\n",
    "\n",
    "    raw_stacked = ds_item[0]\n",
    "    lesion_mask = ds_item[1]\n",
    "    raw_img = raw_stacked[0].numpy().astype(np.uint8)\n",
    "    artery_mask = raw_stacked[1].numpy().astype(np.uint8)\n",
    "    lesion_mask = lesion_mask.numpy().astype(np.uint8)\n",
    "\n",
    "    # Fill the mask\n",
    "    kernelSize = (4,4)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "    artery_mask_repaired = cv2.morphologyEx(artery_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # centerline mask\n",
    "    centerline_mask = skeletonize(artery_mask_repaired).astype(np.uint8)\n",
    "    coordinates = np.where(centerline_mask != 0)\n",
    "    coordinates = np.vstack(coordinates).transpose(1,0)\n",
    "\n",
    "    # randomly sparcify centerline\n",
    "    nb_choice = int(np.ceil(len(coordinates) * 0.5))\n",
    "    indices = np.random.choice(coordinates.shape[0], nb_choice, replace=False)\n",
    "    sparse_centerline  = np.zeros_like(centerline_mask)\n",
    "    sparse_centerline[coordinates[indices][:,0], coordinates[indices][:,1]] = 1\n",
    "    coordinates = np.where(sparse_centerline != 0)\n",
    "    coordinates = np.vstack(coordinates).transpose(1,0)\n",
    "\n",
    "    # generate radiomix features\n",
    "    try:\n",
    "        features, values = generate_radiomix_per_coordinate(extractor, 46, coordinates, raw_img, artery_mask_repaired, 15)\n",
    "        row = (base_path, patient_id, lesion_id_syntax, VOCE, values)\n",
    "        data.append(row)\n",
    "    except a as Exception:\n",
    "        print(\"skipped item with base_path: \", base_path)\n",
    "        print(a)\n",
    "\n",
    "pickle_out = open(\"data/datasets/radiomixOnCenterline/radiomix_data.pkl\",\"wb\")\n",
    "pickle.dump(data, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to create patches from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "\n",
    "# masks = []\n",
    "# for i,j in coordinates:\n",
    "#     ma = gen_radius_mask_2d(raw_img, i, j, 10, artery_mask)\n",
    "#     masks.append(ma)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].imshow(2 * centerline_mask + lesion_mask)\n",
    "\n",
    "# m = np.zeros_like(masks[0])\n",
    "# for ma in masks: m += ma\n",
    "ax[1].imshow(m)\n",
    "# ax[1].imshow(2 * sparse_centerline + lesion_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from skimage.morphology import skeletonize\n",
    "\n",
    "ds_item = fame2_ds[0]\n",
    "\n",
    "raw_stacked = ds_item[0]\n",
    "lesion_mask = ds_item[1]\n",
    "raw_img = raw_stacked[0].numpy().astype(np.uint8)\n",
    "artery_mask = raw_stacked[1].numpy().astype(np.uint8)\n",
    "lesion_mask = lesion_mask.numpy().astype(np.uint8)\n",
    "\n",
    "# Fill holes in the mask\n",
    "kernelSize = (4,4)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "artery_mask_repaired = cv2.morphologyEx(artery_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# centerline mask\n",
    "centerline_mask = skeletonize(artery_mask_repaired).astype(np.uint8)\n",
    "coordinates = np.where(centerline_mask != 0)\n",
    "coordinates = np.vstack(coordinates).transpose(1,0)\n",
    "\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an algo that will iterate coordinates \n",
    "# an start eliminating all of them that are in a radius r\n",
    "# Then it will jump in the closest non eliminated coordinate and do the same\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "\n",
    "def erase_radius(mask, i, j, radius):\n",
    "    assert mask[i,j] == 1\n",
    "    d1l = i-radius if i-radius >= 0 else 0 \n",
    "    d2l = j-radius if j-radius >= 0 else 0 \n",
    "    mask[d1l:i+(radius+1), d2l:j+(radius+1)] = 0\n",
    "    mask[i,j] = 1\n",
    "\n",
    "def refresh_coordinates(mask):\n",
    "    coordinates = np.where(mask != 0)\n",
    "    coordinates = np.vstack(coordinates).transpose(1,0)\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "\n",
    "f,ax = plt.subplots(1,2)\n",
    "\n",
    "# find the start\n",
    "mask = np.copy(centerline_mask)\n",
    "stack = list()\n",
    "r = 10\n",
    "\n",
    "# Start the loop\n",
    "ax[0].imshow(mask)\n",
    "\n",
    "i,j = coordinates[0,0], coordinates[0,1]\n",
    "erase_radius(mask, i, j, r)\n",
    "\n",
    "ax[1].imshow(mask)\n",
    "ax[1].scatter([j], [i], s=5)\n",
    "\n",
    "coordinates = refresh_coordinates(mask)\n",
    "\n",
    "pairwise_distances(coordinates, coordinates[0].reshape(1,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pairwise_distances(coordinates, coordinates[0].reshape(1,-1))\n",
    "\n",
    "np.linalg.norm(coordinates - coordinates[0], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Transformers using patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=False,\n",
    "    duplicate_imgs_with_multiple_lesions=False,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import random\n",
    "\n",
    "def extract_patches(img, img_mask, patch_size, nb_patch, rnd=0, debug=False):\n",
    "    patches = []\n",
    "    \n",
    "    img_shape = img.shape\n",
    "    \n",
    "    # If the patch is too big, extract a smaller one\n",
    "    reduction_ratio = 1\n",
    "    while patch_size >= img_shape[0]//2 or patch_size >= img_shape[1]//2:\n",
    "        patch_size //= 2\n",
    "        reduction_ratio *= 2\n",
    "    \n",
    "    # Do not extract on the border\n",
    "    img_mask[:, :int(patch_size/2)] = 0\n",
    "    img_mask[:int(patch_size/2), :] = 0\n",
    "    img_mask[int(-patch_size/2):, :] = 0\n",
    "    img_mask[:, int(-patch_size/2):] = 0\n",
    "    \n",
    "    # Select uniformly (+ some noise) the position of the extracted patches\n",
    "    x_line = np.linspace(start=patch_size//2+1, stop=img_shape[1]-patch_size-1, num=nb_patch)\n",
    "    x_line += (patch_size//2-1)*(np.random.rand(nb_patch))\n",
    "    \n",
    "    if debug:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img, cmap=cm.gray)\n",
    "        proba_dist_plt = np.copy(img_mask)\n",
    "        proba_dist_plt = 255*proba_dist_plt/np.max(proba_dist_plt)\n",
    "        proba_dist_plt[proba_dist_plt>0] = 255\n",
    "        ax.imshow(proba_dist_plt, cmap=cm.gray, interpolation='none', alpha=0.5)\n",
    "        ax.set_title(\"Proba distribution\")\n",
    "            \n",
    "    for i in range(0, nb_patch):\n",
    "        width_pos = int(x_line[i])\n",
    "        proba_line = img_mask[:,width_pos].numpy() # for some reason choice does not work with tensor\n",
    "        \n",
    "        if random.random() < rnd:\n",
    "            height_pos = patch_size//2+1+int((img_shape[0]-patch_size-1)*random.random())\n",
    "        else:\n",
    "            if np.sum(proba_line)  == 0:\n",
    "                height_pos = patch_size//2+1+int((img_shape[0]-patch_size-1)*random.random())\n",
    "            else:\n",
    "                proba_line /= np.sum(proba_line)\n",
    "                height_pos = np.random.choice(img_shape[0], p=proba_line)\n",
    "\n",
    "        if debug:\n",
    "            plt.plot(width_pos, height_pos, 'ro')\n",
    "            plt.vlines(width_pos, 0, img_shape[0]-1, colors=\"red\", linestyles=\"--\")\n",
    "\n",
    "        extracted_patch = img[int(height_pos-patch_size/2):int(height_pos+patch_size/2), \n",
    "                                        int(width_pos-patch_size/2):int(width_pos+patch_size/2)]\n",
    "        \n",
    "        if reduction_ratio != 1:\n",
    "            resizer = transforms.Resize(extracted_patch.shape[0]*reduction_ratio)\n",
    "            extracted_patch = resizer(extracted_patch.unsqueeze(0).unsqueeze(0))[0,0,:,:]\n",
    "        \n",
    "        patches.append(extracted_patch)\n",
    "        \n",
    "    if debug:\n",
    "        plt.title(\"Patches center position\")\n",
    "        plt.show()\n",
    "        \n",
    "    if debug:\n",
    "        fig, axs = plt.subplots(1, nb_patch)\n",
    "        for i, patch in enumerate(patches):\n",
    "            axs[i].imshow(patch, cmap=\"gray\")\n",
    "            axs[i].set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        fig.show()\n",
    "        plt.show()\n",
    "        \n",
    "    return patches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores all data in a list with format: (base_path, patient_id, lesions_id_syntax, radiomix_features)\n",
    "data = [ ]\n",
    "for ds_item in tqdm(fame2_ds):\n",
    "    # Other features\n",
    "    base_path  = ds_item[2].base_path.values[0]\n",
    "    patient_id = ds_item[2].patient_id_x.values[0]\n",
    "    lesion_id_syntax =  ds_item[2].lesion_id_syntax.values[0]\n",
    "    VOCE =  ds_item[2].Event.values[0]\n",
    "\n",
    "    raw_stacked = ds_item[0]\n",
    "    lesion_mask = ds_item[1]\n",
    "    raw_img = raw_stacked[0].numpy().astype(np.uint8)\n",
    "    artery_mask = raw_stacked[1].numpy().astype(np.uint8)\n",
    "    lesion_mask = lesion_mask.numpy().astype(np.uint8)\n",
    "\n",
    "    # Fill the mask\n",
    "    kernelSize = (4,4)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "    artery_mask_repaired = cv2.morphologyEx(artery_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # centerline mask\n",
    "    centerline_mask = skeletonize(artery_mask_repaired).astype(np.uint8)\n",
    "\n",
    "    # generate radiomix features\n",
    "    try:\n",
    "        # We need 9 patches of 64 so when stacking them up to \n",
    "        # create 1 image that will be fed to ViT\n",
    "        patches = extract_patches(\n",
    "            raw_img, torch.tensor(centerline_mask, dtype=torch.float64), \n",
    "            64, 9, rnd=0.0, debug=False\n",
    "        )\n",
    "        # Stack patches in 1 image ready to be passed to ViT\n",
    "        img = torch.tensor(np.vstack([\n",
    "            np.hstack(patches[:3]),\n",
    "            np.hstack(patches[3:6]),\n",
    "            np.hstack(patches[6:9])\n",
    "        ])).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "        row = (base_path, patient_id, lesion_id_syntax, VOCE, img)\n",
    "        data.append(row)\n",
    "    except a as Exception:\n",
    "        print(\"skipped item with base_path: \", base_path)\n",
    "        print(a)\n",
    "\n",
    "pickle_out = open(\"data/datasets/patchesForVitOnCenterline/data.pkl\",\"wb\")\n",
    "pickle.dump(data, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.pylightning import Fame2GraphSimpleDataModule\n",
    "\n",
    "class Confs:\n",
    "    dataset_dir = 'data/datasets/forecasting/knnOnlyLesionsFull'\n",
    "    train_val_split = 'data/datasets/val_train_split'\n",
    "    standardize_graph=False\n",
    "    standardize_global=True\n",
    "    skip_pixel_intensity=False\n",
    "    use_balanced_batches=True\n",
    "    num_workers=4\n",
    "    train_batch_size=2\n",
    "    m = 0\n",
    "    v = 1\n",
    "    gnn_cls_name='GIN'\n",
    "    gnn_pooling_cls_name=\"CustomPooling\"\n",
    "    gnn_node_features=4\n",
    "    gnn_node_emb_dim=512\n",
    "    gnn_nb_layers=3\n",
    "    gnn_dropout=0.5\n",
    "    gnn_norm=\"BatchNorm\"\n",
    "    gnn_act=\"ReLU\"\n",
    "    gnn_ffr_pooling_factor=2\n",
    "    gnn_ffr_pooling_proj_dim=16\n",
    "    gnn_global_hidden_dim=512\n",
    "    nb_classes=1\n",
    "\n",
    "    use_lesion_wide_info=True\n",
    "    pool_only_lesion_points=False\n",
    "    node_pooling=\"all_stats\"\n",
    "\n",
    "\n",
    "\n",
    "confs = Confs()\n",
    "\n",
    "dm = Fame2GraphSimpleDataModule(confs)\n",
    "dm.setup(None)\n",
    "dl = iter(dm.train_dataloader())\n",
    "b = next(dl)\n",
    "print(\"len of batch\", len(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.networks.gnn as gnn\n",
    "\n",
    "\n",
    "net = gnn.GNNClassifier(\n",
    "        gnn.gnn_resolver(confs),\n",
    "        gnn.ConfigurablePooling(confs), \n",
    "        confs\n",
    ")\n",
    "\n",
    "net(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.dataset import *\n",
    "import os.path as op\n",
    "\n",
    "graphDsWrapper = Fame2GraphDatasetWrapper()\n",
    "train_ds = graphDsWrapper.instantiate_Fame2GraphDataset(None, None, root=op.join(\"data/datasets/forecasting/knn5Inner05OnlyLesionsFull\", \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([[1,0],[0,2], [1,1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = train_ds[0].x\n",
    "edge_index = train_ds[0].edge_index.transpose(1,0)\n",
    "coord = torch.vstack([x[:,0], x[:,1]]).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_graph(coordinates, edge_index):\n",
    "    g = ig.Graph(len(coordinates))\n",
    "    g.vs['x'] = coordinates[:,0].tolist()\n",
    "    g.vs['y'] = coordinates[:,1].tolist()\n",
    "\n",
    "    coords = g.layout_auto().coords\n",
    "    for edge in edge_index:\n",
    "        g.add_edge(edge[0], edge[1])\n",
    "    g.simplify() \n",
    "\n",
    "    return g\n",
    "\n",
    "f,a = plt.subplots()\n",
    "g = create_graph(coord, edge_index)\n",
    "ig.plot(\n",
    "    g,\n",
    "    target=a,\n",
    "    vertex_size=0.04,\n",
    "    vertex_color=\"lightblue\",\n",
    "    edge_width=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.networks import baseline\n",
    "\n",
    "class Confs:\n",
    "    cnn_dropout = 0.5\n",
    "    weight_init_strategy=\"Xavier Uniform\"\n",
    "net = baseline.BaselineResNet( Confs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cnn2Gnn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=False,\n",
    "    duplicate_imgs_with_multiple_lesions=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)\n",
    "\n",
    "fame2_ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelaunayVertexConnector:\n",
    "    def __init__(self, distance_cuttoff) -> None:\n",
    "        super().__init__()\n",
    "        self.distance_cutoff = distance_cuttoff\n",
    "        self.store_pruned_simplices = []\n",
    "\n",
    "    def connect(self, coordinates: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        triangulation = Delaunay(coordinates)\n",
    "        return self.to_edges_coo(self.prune_simplices(triangulation, coordinates))\n",
    "\n",
    "    def prune_simplices(self, triangulation, points):\n",
    "        idxes_to_keep = []\n",
    "        cnt = 0\n",
    "        cond = lambda di: di > self.distance_cutoff\n",
    "        for i, row in enumerate(points[triangulation.simplices]):\n",
    "            bitmap = [0,0,0]\n",
    "            bitmap[0] = cond(torch.norm((row[0]- row[1]).to(torch.float)))\n",
    "            bitmap[1] = cond(torch.norm((row[0]- row[2]).to(torch.float)))\n",
    "            bitmap[2] = cond(torch.norm((row[2]- row[1]).to(torch.float)))\n",
    "            if sum(bitmap) != 0: \n",
    "                cnt\n",
    "                continue\n",
    "            idxes_to_keep.append(i)\n",
    "        self.store_pruned_simplices.append(cnt)\n",
    "        return triangulation.simplices[idxes_to_keep]\n",
    "\n",
    "    def to_edges_coo(self, simplices):\n",
    "        edge_indexes = set()\n",
    "        for tri in simplices:\n",
    "            edge_indexes.add((tri[0], tri[1]))\n",
    "            edge_indexes.add((tri[1], tri[0]))\n",
    "            edge_indexes.add((tri[0], tri[2]))\n",
    "            edge_indexes.add((tri[2], tri[0]))\n",
    "            edge_indexes.add((tri[1], tri[2]))\n",
    "            edge_indexes.add((tri[2], tri[1]))\n",
    "        return torch.LongTensor(list(edge_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Create the dataset\n",
    "conn = DelaunayVertexConnector(10)\n",
    "\n",
    "data = []\n",
    "for ds_item in tqdm(fame2_ds):\n",
    "    # Other features\n",
    "    base_path  = ds_item[2].base_path.values[0]\n",
    "    patient_id = ds_item[2].patient_id_x.values[0]\n",
    "    lesion_id_syntax =  ds_item[2].lesion_id_syntax.values[0]\n",
    "    VOCE =  ds_item[2].Event.values[0]\n",
    "\n",
    "    img = ds_item[0][0]\n",
    "    mask = ds_item[0][1].to(torch.bool)\n",
    "    coordinates = torch.vstack(torch.where(mask == True))\n",
    "    edge_index = conn.connect(coordinates.transpose(1,0), None)\n",
    "\n",
    "    data.append((img, mask, edge_index, base_path, patient_id, lesion_id_syntax, VOCE ))\n",
    "\n",
    "\n",
    "pickle_out = open(\"data/datasets/eventForecastingCnn2Gnn/data.pkl\",\"wb\")\n",
    "pickle.dump(data, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "print(np.mean(conn.store_pruned_simplices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test cnn2gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.dataset import Cnn2GnnDataset, SplitType\n",
    "from cardio.pylightning.dm_fame2_graph import Fame2GraphKFoldDataModule\n",
    "\n",
    "class Confs:\n",
    "    dataset_dir = \"data/datasets/lesionDetection/Delaunay5_Inner050_Alwayscenter\"\n",
    "    use_balanced_batches = \"True\"\n",
    "    num_workers = 4\n",
    "    train_batch_size = 2\n",
    "    test_batch_size=2\n",
    "    standardize_graph=True\n",
    "    standardize_global=True\n",
    "    device=\"cpu\"\n",
    "    use_lesion_wide_info=True\n",
    "    skip_pixel_intensity=True\n",
    "    \n",
    "\n",
    "dm = Fame2GraphKFoldDataModule(Confs())\n",
    "dm.setup(None)\n",
    "dm.setup_folds(5)\n",
    "dm.setup_fold_index(0)\n",
    "ds = dm.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Cnn2GnnDataset(\"data/datasets/eventForecastingCnn2Gnn\", SplitType.train, split_path=\"data/datasets/eventForecastingCnn2Gnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ds[100][0].flatten(), bins = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = np.random.choice(len(ds), int(len(ds)*0.2))\n",
    "ds_v = ds.index_select(idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.networks.cnn2gnn import Cnn2Gnn\n",
    "class Confs:\n",
    "    gnn_node_emb_dim=512\n",
    "    gnn_nb_layers = 3\n",
    "    gnn_node_features = 32\n",
    "    nb_classes=1\n",
    "    gnn_dropout = 0.5\n",
    "    gnn_cls_name = \"CustomGIN\"\n",
    "\n",
    "    cnn_out_channels = 64\n",
    "    cnn_kernel = 2\n",
    "\n",
    "b = next(iter(dm.train_dataloader()))\n",
    "net = Cnn2Gnn(Confs())\n",
    "net(b[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the CNN dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardio.resolvers as resolvers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cardio.dataset import SplitType\n",
    "from cardio.configuration import *\n",
    "from cardio.dataset import Fame2RawDSLoader\n",
    "from cardio.dataset.fame2_raw_data_loader import load_train_test_split\n",
    "\n",
    "fame2_ds = Fame2RawDSLoader(\n",
    "    FAME2_DUMP_DIR, \n",
    "    FILEPATH_CLINICAL_EVENT_DF, \n",
    "    only_lesion_data_points=True,\n",
    "    generate_data_with_2_views=False,\n",
    "    duplicate_imgs_with_multiple_lesions=True,\n",
    "    df_ce_event_columns=EVENT_COLUMNS,\n",
    "    generate_lesion_labels=True, \n",
    "    lazy_load=True,\n",
    "    all_arteries_in_one_img=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.df.patient_id_x.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "113/562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "450 + 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.setup(\"data/datasets/simpleSplit/\", SplitType.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.setup(\"data/datasets/simpleSplit/\", SplitType.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fame2_ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.dataset.preprocess import graph\n",
    "from tqdm import tqdm\n",
    "from cardio.dataset.fame2_patch_dataset import create_ds_split\n",
    "\n",
    "save_dir = \"data/datasets/cnn/patches_200x200_GlobFeat\"\n",
    "\n",
    "features = [\"DS\", \"FFR\", \"Lesion_Length\",\n",
    "            \"Lesion_Type\", \"artery\", \"exactSegmentLocation\"]\n",
    "feature_transforms = [None, None, None,\n",
    "    graph.lesion_type_to_1hot,\n",
    "    graph.artery_to_1hot,\n",
    "    graph.artery_segment_to_float\n",
    "]\n",
    "\n",
    "# Generate the dataset\n",
    "fame2_ds.setup(SPLIT_PATH, SplitType.train)\n",
    "index, data = zip(*fame2_ds.data)\n",
    "df = pd.concat([d for d in data])\n",
    "df.index = index\n",
    "create_ds_split(df, True, features, feature_transforms, save_dir)\n",
    "\n",
    "fame2_ds.setup(SPLIT_PATH, SplitType.test)\n",
    "index, data = zip(*fame2_ds.data)\n",
    "df = pd.concat([d for d in data])\n",
    "df.index = index\n",
    "create_ds_split(df, False, features, feature_transforms, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the lesion wide dataset\n",
    "from cardio.dataset.fame2_patch_dataset import Fame2CNNDataset\n",
    "from cardio.dataset import SplitType\n",
    "\n",
    "ds = Fame2CNNDataset(\"data/datasets/cnn/patches_200x200_GlobFeat\", SplitType.test, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i=200\n",
    "f,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(ds[i][0][0][0,:])\n",
    "ax[1].imshow(ds[i][0][0][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.pylightning.dm_fame2_baseline import Fame2BaselineKFold \n",
    "\n",
    "class Confs:\n",
    "    n = 0\n",
    "    dataset_dir=\"data/datasets/cnn/patches_200x200_GlobFeat\"\n",
    "    num_workers = 4\n",
    "    train_batch_size = 2\n",
    "    standardize_global=True\n",
    "    use_lesion_wide_info = True\n",
    "    use_balanced_batches = True\n",
    "    standardize_img = True\n",
    "    skip_pixel_intensity = True\n",
    "    weight_init_strategy='pre_trained'\n",
    "    device = \"cpu\"\n",
    "    \n",
    "\n",
    "confs = Confs()\n",
    "dm = Fame2BaselineKFold(confs)\n",
    "dm.setup(None)\n",
    "dm.setup_folds(5)\n",
    "dm.setup_fold_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.pylightning.dm_fame2_graph import Fame2GraphSimpleDataModule \n",
    "\n",
    "class Confs:\n",
    "    n = 0\n",
    "    dataset_dir=\"data/datasets/forecasting/Delaunay5_Inner050_Alwayscenter_OnlyLesions_FUL/\"\n",
    "    num_workers = 4\n",
    "    train_batch_size = 2\n",
    "    standardize_global=True\n",
    "    use_lesion_wide_info = True\n",
    "    use_balanced_batches = True\n",
    "    standardize_graph = True\n",
    "    skip_pixel_intensity = True\n",
    "    dont_validate=True\n",
    "    device = \"cpu\"\n",
    "    \n",
    "\n",
    "confs = Confs()\n",
    "dm = Fame2GraphSimpleDataModule(confs)\n",
    "dm.setup(None)\n",
    "# dm.setup_folds(5)\n",
    "# dm.setup_fold_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = np.arange(10)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardio.networks.cnn_baseline import BaselineResNet\n",
    "from cardio.resolvers import  gnn_resolver\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "confs.gnn_cls_name = \"GIN\"\n",
    "confs.gnn_node_emb_dim = 256\n",
    "confs.gnn_nb_layers = 5\n",
    "confs.gnn_act = 'ReLU'\n",
    "confs.gnn_norm = \"BatchNorm\"\n",
    "confs.gnn_pooling_cls_name = \"ConfigurablePooling\"\n",
    "confs.pool_only_lesion_points = False\n",
    "confs.gnn_is_siamese = False\n",
    "confs.node_pooling = \"all_stats\"\n",
    "confs.gnn_use_global_info = True\n",
    "confs.gnn_dropout = 0.2\n",
    "confs.nb_classes = 1\n",
    "confs.cnn_dropout = 0.5\n",
    "net = gnn_resolver(confs)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
