{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, GATConv, GATv2Conv, GINConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn.models.basic_gnn import BasicGNN\n",
    "from torch_geometric.nn.models.basic_gnn import *\n",
    "import random\n",
    "#\n",
    "# Architecture abstractions\n",
    "#\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, GATConv, GATv2Conv, GINConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn.models.basic_gnn import BasicGNN\n",
    "from torch_geometric.nn.models.basic_gnn import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Architecture abstractions\n",
    "#\n",
    "\n",
    "class GNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, gnn: nn.Module, pooling_layer: nn.Module, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.gnn = gnn\n",
    "        self.pooling = pooling_layer\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        '''\n",
    "            Params\n",
    "            ---\n",
    "            * `batched_data`: The batch as outputted from torch_geometric dataloader\n",
    "        '''\n",
    "        # We might pass more features that gnn_node features\n",
    "        # Those would be aux features for the pooling layer\n",
    "        batched_x = batched_data.x[:, :self.cfg.gnn_node_features]\n",
    "        aux_feats = batched_data.x[:, self.cfg.gnn_node_features:]\n",
    "\n",
    "        # standardize x. Sometimes m,v are 0 and 1 so no std happens\n",
    "        batched_x = (batched_x - self.cfg.m)/self.cfg.v\n",
    "\n",
    "        args = {}\n",
    "        if hasattr(batched_data, 'siamese_ptr'):\n",
    "            args['siamese_ptr'] = batched_data.siamese_ptr\n",
    "        if hasattr(batched_data, 'lesion_wide_feat_tensor'):\n",
    "            args['lesion_wide_feat_tensor'] = batched_data.lesion_wide_feat_tensor\n",
    "\n",
    "        out = self.gnn(batched_x, batched_data.edge_index)\n",
    "        out = self.pooling(out, batched_data.batch, aux_feats, **args)\n",
    "        return out\n",
    "\n",
    "    def prep_predictions(self, pred, to_cpu:bool=True):\n",
    "        ''' Prepare predictions for validation bases on model structure '''\n",
    "        if self.cfg.nb_classes > 1:\n",
    "            return torch.argmax(torch.softmax(pred.cpu().detach(), dim=1), dim=1)\n",
    "        else:\n",
    "            return torch.sigmoid(pred.cpu().detach())\n",
    "\n",
    "#\n",
    "# GNN's\n",
    "#\n",
    "\n",
    "\n",
    "def gnn_resolver(cfg) -> BasicGNN:\n",
    "    import sys\n",
    "    gnn_class_name = getattr(sys.modules[__name__], cfg.gnn_cls_name)\n",
    "\n",
    "\n",
    "    # TODO add also our models\n",
    "    # Dont add output channels since we will pool after\n",
    "    return gnn_class_name(\n",
    "        in_channels = cfg.gnn_node_features,\n",
    "        hidden_channels = cfg.gnn_node_emb_dim,\n",
    "        num_layers = cfg.gnn_nb_layers,\n",
    "        dropout = cfg.gnn_dropout,\n",
    "        act = cfg.gnn_act,\n",
    "        norm = cfg.gnn_norm\n",
    "    )\n",
    "\n",
    "\n",
    "#\n",
    "#  Pooling\n",
    "#\n",
    "\n",
    "class SiamesePoolingWithFFR(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        # Create one node_pooling and then a linear for classification\n",
    "        layers = []\n",
    "        factor = cfg.gnn_ffr_pooling_factor\n",
    "        min_dim = cfg.gnn_ffr_pooling_proj_dim\n",
    "        dim = 2 * cfg.gnn_global_hidden_dim\n",
    "        if factor == 1:\n",
    "            layers.append(nn.Linear(dim, min_dim))\n",
    "            layers.extend([nn.BatchNorm1d(min_dim), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "            dim = min_dim\n",
    "        else:\n",
    "            while dim//factor >= min_dim:\n",
    "                layers.append(nn.Linear(dim, dim//factor))\n",
    "                layers.extend([nn.BatchNorm1d(dim//factor), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "                dim //= factor\n",
    "\n",
    "        # Create one node_pooling and then a linear for classification\n",
    "        self.node_pooling = PoolingLayer(cfg.gnn_node_emb_dim, cfg.gnn_global_hidden_dim)\n",
    "        self.projection = nn.Sequential(*layers)\n",
    "        self.final_layer = nn.Linear(dim+1, cfg.nb_classes)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _ffr_per_batch(aux_feats: torch.Tensor, batch_indexes: torch.Tensor, lesion_mask: torch.Tensor):\n",
    "        prev_val = 1.2 # ffr goes until 1.0\n",
    "        idx = 0\n",
    "        ffr_measures = torch.zeros((batch_indexes.max() + 1,1)).to(aux_feats.device)\n",
    "        for v in aux_feats[lesion_mask]:\n",
    "            if v.item() != prev_val:\n",
    "                ffr_measures[idx] = v\n",
    "                idx +=1\n",
    "                prev_val = v.item()\n",
    "        return ffr_measures\n",
    "\n",
    "    def forward(self, x:torch.Tensor, batch_indexes: torch.Tensor, aux_feats: Optional[torch.Tensor] = None, **kwargs):\n",
    "        '''\n",
    "            Params\n",
    "            ---\n",
    "            * `x`: Thea batched embedding representation from the gnn: Size: [ |Nodes in batch| x | Node Embedding Dim | ]\n",
    "            * `batch_indexes`: A tensor with size: | Nodes in Batch | that has 0,1 values based on which nodes appears in what batch\n",
    "            * `aux_feats`: A tensor with size: [|Nodes in Batch| x 1 ]. The value on each item is FFR or 0.0 if that point is not a lesion.\n",
    "        '''\n",
    "        # Re create batch_indexes\n",
    "        assert 'siamese_ptr' in kwargs, \"No siamese ptrs\"\n",
    "        siamese_ptr = kwargs['siamese_ptr']\n",
    "        new_batch_indexes = torch.zeros(x.shape[0], dtype=torch.int64, device=batch_indexes.device)\n",
    "        curr_idx = 0\n",
    "        curr_dpoints = 0\n",
    "        for ptrs in siamese_ptr:\n",
    "            range_v1_st, range_v1_end = curr_dpoints, ptrs[1] + curr_dpoints\n",
    "            range_v2_st, range_v2_end = range_v1_end + 1, ptrs[2] + curr_dpoints\n",
    "            curr_dpoints =  range_v2_end + 1\n",
    "\n",
    "            new_batch_indexes[range_v1_st:range_v1_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "            new_batch_indexes[range_v2_st:range_v2_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "\n",
    "        # get ffr per batch\n",
    "        lesion_mask = (aux_feats != 0).view(-1)\n",
    "        ffr_per_batch = self._ffr_per_batch(aux_feats, batch_indexes, lesion_mask)\n",
    "\n",
    "        # Pool all global vectors\n",
    "        x_out = self.node_pooling(x, new_batch_indexes)\n",
    "        # concat siamese vectors (every 2 vectors are siamese)\n",
    "        x_out = x_out.view(-1, 2 * self.cfg.gnn_global_hidden_dim)\n",
    "\n",
    "        out = F.relu(x_out)\n",
    "        out = self.projection(out)\n",
    "        out = torch.hstack([out, ffr_per_batch])\n",
    "        out = self.final_layer(out)\n",
    "        return out\n",
    "\n",
    "class SiamesePooling(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        # Create one node_pooling and then a linear for classification\n",
    "        layers = []\n",
    "        factor = cfg.gnn_ffr_pooling_factor\n",
    "        min_dim = cfg.gnn_ffr_pooling_proj_dim\n",
    "        dim = 2 * cfg.gnn_global_hidden_dim\n",
    "        if factor == 1:\n",
    "            layers.append(nn.Linear(dim, min_dim))\n",
    "            layers.extend([nn.BatchNorm1d(min_dim), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "            dim = min_dim\n",
    "        else:\n",
    "            while dim//factor >= min_dim:\n",
    "                layers.append(nn.Linear(dim, dim//factor))\n",
    "                layers.extend([nn.BatchNorm1d(dim//factor), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "                dim //= factor\n",
    "        layers.append(nn.Linear(dim, cfg.nb_classes))\n",
    "\n",
    "        # Create one node_pooling and then a linear for classification\n",
    "        self.node_pooling = PoolingLayer(cfg.gnn_node_emb_dim, cfg.gnn_global_hidden_dim)\n",
    "        self.final_linear= nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, batch_indexes: torch.Tensor, aux_feats: Optional[torch.Tensor] = None, **kwargs):\n",
    "        '''\n",
    "            Params\n",
    "            ---\n",
    "            * `x`: Thea batched embedding representation from the gnn: Size: [ |Nodes in batch| x | Node Embedding Dim | ]\n",
    "            * `batch_indexes`: A tensor with size: | Nodes in Batch | that has 0,1 values based on which nodes appears in what batch\n",
    "            * `aux_feats`: A tensor with size: [|Nodes in Batch| x 1 ]. The value on each item is FFR or 0.0 if that point is not a lesion.\n",
    "        '''\n",
    "        # Re create batch_indexes\n",
    "        assert 'siamese_ptr' in kwargs, \"No siamese ptrs\"\n",
    "        siamese_ptr = kwargs['siamese_ptr']\n",
    "        new_batch_indexes = torch.zeros(x.shape[0], dtype=torch.int64, device=batch_indexes.device)\n",
    "        curr_idx = 0\n",
    "        curr_dpoints = 0\n",
    "        for ptrs in siamese_ptr:\n",
    "            range_v1_st, range_v1_end = curr_dpoints, ptrs[1] + curr_dpoints\n",
    "            range_v2_st, range_v2_end = range_v1_end + 1, ptrs[2] + curr_dpoints\n",
    "            curr_dpoints =  range_v2_end + 1\n",
    "\n",
    "            new_batch_indexes[range_v1_st:range_v1_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "            new_batch_indexes[range_v2_st:range_v2_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "\n",
    "        # Pool all global vectors\n",
    "        x_out = self.node_pooling(x, new_batch_indexes)\n",
    "        # concat siamese vectors (every 2 vectors are siamese)\n",
    "        x_out = x_out.view(-1, 2 * self.cfg.gnn_global_hidden_dim)\n",
    "\n",
    "        out = F.relu(x_out)\n",
    "        out = self.final_linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LesionPoolingWithClinicalData(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Crate decreasing list of models\n",
    "        layers = []\n",
    "        factor = cfg.gnn_ffr_pooling_factor\n",
    "        min_dim = cfg.gnn_ffr_pooling_proj_dim\n",
    "        dim = cfg.gnn_global_hidden_dim\n",
    "        if factor == 1:\n",
    "            layers.append(nn.Linear(dim, min_dim))\n",
    "            layers.extend([nn.BatchNorm1d(min_dim), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "            dim = min_dim\n",
    "        else:\n",
    "            while dim//factor >= min_dim:\n",
    "                layers.append(nn.Linear(dim, dim//factor))\n",
    "                layers.extend([nn.BatchNorm1d(dim//factor), nn.ReLU(), nn.Dropout(cfg.gnn_dropout)])\n",
    "                dim //= factor\n",
    "\n",
    "        # Create one node_pooling and then a linear for classification\n",
    "        self.node_pooling = PoolingLayer(cfg.gnn_node_emb_dim, cfg.gnn_global_hidden_dim)\n",
    "        self.project = nn.Sequential(*layers)\n",
    "        self.final_linear = nn.Sequential(\n",
    "            nn.Linear(dim + cfg.nb_clinical_data_features, 256),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(64, cfg.nb_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor, batch_indexes: torch.Tensor, aux_feats: Optional[torch.Tensor] = None, **kwargs):\n",
    "        '''\n",
    "            Params\n",
    "            ---\n",
    "            * `x`: Thea batched embedding representation from the gnn: Size: [ |Nodes in batch| x | Node Embedding Dim | ]\n",
    "            * `batch_indexes`: A tensor with size: | Nodes in Batch | that has 0,1 values based on which nodes appears in what batch\n",
    "            * `aux_feats`: TODO\n",
    "        '''\n",
    "        assert 'lesion_wide_feat_tensor' in kwargs, \"No lesion_wide_feat_tensor\"\n",
    "        clinical_data = kwargs['lesion_wide_feat_tensor']\n",
    "        x_out = self.node_pooling(x, batch_indexes)\n",
    "        x_out = self.project(x_out)\n",
    "\n",
    "        # put ffr in the features\n",
    "        out = torch.hstack([x_out, clinical_data])\n",
    "        out = self.final_linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Multi purpose pooling Layers\n",
    "#\n",
    "\n",
    "\n",
    "def all_statistics_pooling(features, batch):\n",
    "    assert features.shape[0] == len(batch)\n",
    "    m = global_mean_pool(features, batch)\n",
    "    ma = global_max_pool(features, batch)\n",
    "    mi = -global_max_pool(-features, batch)\n",
    "    std = global_mean_pool(features ** 2, batch) - global_mean_pool(features, batch) ** 2\n",
    "    return torch.hstack((m, mi, ma, std))\n",
    "\n",
    "def resolve_node_pooling(node_pooling):\n",
    "    if node_pooling == \"all_stats\":\n",
    "        return all_statistics_pooling, 4\n",
    "    elif node_pooling == \"sum\":\n",
    "        return global_add_pool, 1\n",
    "    elif node_pooling == \"max\":\n",
    "        return global_max_pool, 1\n",
    "    elif node_pooling == \"mean\":\n",
    "        return global_mean_pool, 1\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown node pooling type: {node_pooling}\")\n",
    "\n",
    "class ConfigurablePooling(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.use_lesion_mask = cfg.pool_only_lesion_points\n",
    "        self.node_pooling, pooling_mul = resolve_node_pooling(cfg.node_pooling)\n",
    "        self.gnn_use_global_info = cfg.use_lesion_wide_info\n",
    "        self.is_siamese = cfg.gnn_is_siamese\n",
    "        self.gnn_node_emb_dim = cfg.gnn_node_emb_dim\n",
    "        in_dim = cfg.gnn_node_emb_dim\n",
    "        out_dim = cfg.nb_classes\n",
    "        global_info_dim = cfg.lesion_wide_feat_dim if self.gnn_use_global_info else 0\n",
    "        siamese_mul = 2 if self.is_siamese else 1\n",
    "        self.siamese_reshaping = in_dim * pooling_mul\n",
    "\n",
    "        self.lin1= nn.Sequential(\n",
    "            nn.Linear(in_dim * pooling_mul * siamese_mul, 256),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "        )\n",
    "\n",
    "        # lin2 might also get global info concatenated\n",
    "        self.lin2= nn.Sequential(\n",
    "            nn.Linear(64 + global_info_dim, 32),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(), nn.Dropout(cfg.gnn_dropout),\n",
    "            nn.Linear(16, out_dim)\n",
    "        )\n",
    "\n",
    "    def __siamese_batching(self, siamese_ptr, node_embs, batch_indexes):\n",
    "        ''' Change the batch indexes to pool information for the node embeddings of both graphs. '''\n",
    "        new_batch_indexes = torch.zeros(node_embs.shape[0], dtype=torch.int64, device=batch_indexes.device)\n",
    "        curr_idx = 0\n",
    "        curr_dpoints = 0\n",
    "        for ptrs in siamese_ptr:\n",
    "            range_v1_st, range_v1_end = curr_dpoints, ptrs[1] + curr_dpoints\n",
    "            range_v2_st, range_v2_end = range_v1_end + 1, ptrs[2] + curr_dpoints\n",
    "            curr_dpoints =  range_v2_end + 1\n",
    "\n",
    "            new_batch_indexes[range_v1_st:range_v1_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "            new_batch_indexes[range_v2_st:range_v2_end+1] = curr_idx\n",
    "            curr_idx += 1\n",
    "\n",
    "        return new_batch_indexes\n",
    "\n",
    "    def forward(self, node_embs, batch_indexes, lesion_mask = None, lesion_wide_feat_tensor=None, siamese_ptr=None):\n",
    "        # lesion masking if requested\n",
    "        if self.use_lesion_mask:\n",
    "           node_embs = node_embs[lesion_mask]\n",
    "           batch_indexes = batch_indexes[lesion_mask]\n",
    "\n",
    "        # handle siamese\n",
    "        if self.is_siamese:\n",
    "            assert siamese_ptr != None\n",
    "            batch_indexes = self.__siamese_batching(siamese_ptr, node_embs, batch_indexes)\n",
    "\n",
    "\n",
    "        # node pooling\n",
    "        out = self.node_pooling(node_embs, batch_indexes)\n",
    "\n",
    "        # if siamese\n",
    "        if self.is_siamese:\n",
    "            # concat siamese vectors (every 2 vectors are siamese)\n",
    "            out = out.view(-1, 2 * self.siamese_reshaping)\n",
    "\n",
    "        # projection\n",
    "        out = self.lin1(out)\n",
    "\n",
    "        # global info attachment\n",
    "        if self.gnn_use_global_info:\n",
    "            out = torch.hstack([out, lesion_wide_feat_tensor])\n",
    "\n",
    "        # classifier\n",
    "        return self.lin2(out)\n",
    "\n",
    "\n",
    "def pooling_resolver(cfg) -> nn.Module:\n",
    "    if cfg.gnn_pooling_cls_name == \"SiamesePooling\":\n",
    "        return SiamesePooling(cfg)\n",
    "    elif cfg.gnn_pooling_cls_name == \"SiamesePoolingWithFFR\":\n",
    "        return SiamesePoolingWithFFR(cfg)\n",
    "    elif cfg.gnn_pooling_cls_name == \"ConfigurablePooling\":\n",
    "        return ConfigurablePooling(cfg)\n",
    "    else:\n",
    "        raise RuntimeError(f\"We din't have {cfg.gnn_pooling_cls_name} in pooling layers.\")\n",
    "\n",
    "\n",
    "#\n",
    "# Util Layers\n",
    "#\n",
    "\n",
    "class PoolingLayer(nn.Module):\n",
    "    def __init__(self, d, global_hidden_dim):\n",
    "        \"\"\" Pool node or edge features to graph level features. \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(4 * d, global_hidden_dim),\n",
    "                nn.BatchNorm1d(global_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(global_hidden_dim, global_hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, features, batch):\n",
    "        assert features.shape[0] == len(batch)\n",
    "        m = global_mean_pool(features, batch)\n",
    "        ma = global_max_pool(features, batch)\n",
    "        mi = -global_max_pool(-features, batch)\n",
    "        std = global_mean_pool(features ** 2, batch) - global_mean_pool(features, batch) ** 2\n",
    "        z = torch.hstack((m, mi, ma, std))\n",
    "        out = self.lin(z)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--lesion_wide_feat_dim'], dest='lesion_wide_feat_dim', nargs=None, const=None, default=21, type=<class 'int'>, choices=None, required=False, help='The number of clinical data features, if used with ClinicalDataPooling', metavar=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def parse_args_():\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--nb_classes\", type=int, default=1, help=\"The number of classes to predict for our clf task\")\n",
    "\n",
    "\n",
    "\n",
    "# This is automatically set from the dataset module\n",
    "parser.add_argument(\"--gnn_node_features\", type=int, default=3, help=\"Dim of features in the nodes that we will feed to GNN\")\n",
    "\n",
    "# CNN model parameters\n",
    "parser.add_argument(\"--standardize_img\", action='store_true', help=\"Standardizes the input of the GNN\")\n",
    "parser.add_argument(\"--cnn_dropout\", type=float, default=0.45081799988646554, help=\"The dropout of the CNN fully connected layer at the end\")\n",
    "\n",
    "# CNN model that inputs data to GNN\n",
    "parser.add_argument(\"--cnn_out_channels\", type=int, default=64, help=\"The num of features the CNN needs to create, and pass to GNN a channels\")\n",
    "parser.add_argument(\"--cnn_kernel\", type=int, default=3, help=\"The kernel to use for the CNN\")\n",
    "\n",
    "# Transformer for radiomix model parameters\n",
    "parser.add_argument(\"--tr_nb_radiomix_features\", type=int, default=46, help=\"The number of different radiomix features we use\")\n",
    "parser.add_argument(\"--tr_d_model\", type=int, default=512, help=\"The dim of the transformer embeddings\")\n",
    "parser.add_argument(\"--tr_nhead\", type=int, default=8, help=\"The number of heads\")\n",
    "parser.add_argument(\"--tr_dropout\", type=float, default=0.1, help=\"The number of heads\")\n",
    "parser.add_argument(\"--tr_nb_layers\", type=int, default=6, help=\"The number of layers on the encoder\")\n",
    "parser.add_argument(\"--tr_dim_feedforward\", type=int, default=2048, help=\"The feed forward dimension on the encode layers\")\n",
    "\n",
    "# GNN model parameters\n",
    "# -- pooling\n",
    "parser.add_argument(\"--gnn_pooling_cls_name\", type=str, default=\"ConfigurablePooling\", help=\"One of pooling layers define din 'cardio.networks.gnn' \")\n",
    "parser.add_argument(\"--pool_only_lesion_points\", action='store_true', help=\"In case we use whole artery, gnn pooling will pool only lesion points\")\n",
    "parser.add_argument(\"--node_pooling\", type=str, default='mean', help=\"How to pool the nodes of the gnn: sum, mean, max, all_stats\")\n",
    "# -- model\n",
    "parser.add_argument(\"--gnn_is_siamese\", default=False, help=\"In case we use siamese dataset use this flag\")\n",
    "parser.add_argument(\"--gnn_cls_name\", type=str, default=\"GIN\", help=\"One of the pred class names in 'torch_geometric.nn.models.basic_gnn' or one of our custom gnn class names, CustomGIN\")\n",
    "parser.add_argument(\"--gnn_node_emb_dim\", type=int, default=256, help=\"GNN hidden dim that will be input of the GATconv\")\n",
    "parser.add_argument(\"--gnn_global_hidden_dim\", type=int, default=512, help=\"GNN The dimension to have after the pooling \")\n",
    "parser.add_argument(\"--gnn_nb_layers\", type=int, default=6, help=\"GNN number of Transformer layers\")\n",
    "parser.add_argument(\"--gnn_dropout\", type=float, default=0.415081799988646554, help=\"GNN The dropout on the Attention layers of the gnn\")\n",
    "parser.add_argument(\"--gnn_act\", type=str, default=\"relu\", help=\"The type of activation (one of torch activations)\")\n",
    "parser.add_argument(\"--gnn_norm\", type=str, default=\"BatchNorm\", help=\"One of the normalizations, see 'torch_geometric.nn.norm.__init__.py'\")\n",
    "parser.add_argument(\"--gnn_freeze_weights\", type=bool, default=False, help=\"Freeze gnn weights (not pooling)\")\n",
    "parser.add_argument(\"--gnn_ffr_pooling_factor\", type=int, default=1, help=\"Scale the global dim by a factor multiple times (until dim is >= 16) before you concat it with ffr\")\n",
    "parser.add_argument(\"--gnn_ffr_pooling_proj_dim\", type=int, default=16, help=\"Stops scalling when you reach this projection dimention\")\n",
    "parser.add_argument(\"--use_lesion_wide_info\", default=True,  help=\"Uses lesion wide features like FFR and DS\")\n",
    "# parser.add_argument(\"--nb_clinical_data_features\", default=24, type=int, help=\"The number of clinical data features, if used with ClinicalDataPooling\")\n",
    "parser.add_argument(\"--lesion_wide_feat_dim\", default=21, type=int, help=\"The number of clinical data features, if used with ClinicalDataPooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "model structure is: \n",
      "GNNClassifier(\n",
      "  (gnn): GIN(3, 256, num_layers=6)\n",
      "  (pooling): ConfigurablePooling(\n",
      "    (lin1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU()\n",
      "      (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "    )\n",
      "    (lin2): Sequential(\n",
      "      (0): Linear(in_features=85, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU()\n",
      "      (11): Dropout(p=0.4150817999886465, inplace=False)\n",
      "      (12): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "conf = parser.parse_args(args=[])\n",
    "print(conf.gnn_nb_layers)\n",
    "\n",
    "model = GNNClassifier(gnn_resolver(conf),\n",
    "                    pooling_resolver(conf), conf)\n",
    "print(\"model structure is: \")\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
